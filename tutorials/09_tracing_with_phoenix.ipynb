{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3519d054",
   "metadata": {},
   "source": [
    "# Tutorial 9 - Tracing LLM Workflows with Arize Phoenix\n",
    "\n",
    "## Where You Are in the Learning Journey\n",
    "\n",
    "```\n",
    " Tutorials 1-5      Tutorials 6-8      Tutorial 9\n",
    " RAG Fundamentals   Agent Extension    Observability\n",
    " (retrieval         (ReAct, Reflect,   and Tracing\n",
    "  pipeline)          State Mgmt)        (you are here)\n",
    "```\n",
    "\n",
    "**What this tutorial adds:** visibility into what is happening *inside* the\n",
    "LLM workflow at runtime. Every retrieval call, every LLM generation, and every\n",
    "agent step produces a structured record called a **span**. A group of related\n",
    "spans is called a **trace**. You will capture, inspect, and display those traces\n",
    "both in-notebook and inside a running Arize Phoenix server.\n",
    "\n",
    "**What you will learn in this tutorial:**\n",
    "- What tracing is and why it matters for LLM workflows\n",
    "- What a span is: the atomic unit of a trace\n",
    "- How OpenTelemetry provides a standard API for capturing spans\n",
    "- How to trace retrieval, generation, and agent steps step by step\n",
    "- How Arize Phoenix visualises traces in a browser UI\n",
    "- How to use auto-instrumentation so OpenAI calls are traced automatically\n",
    "\n",
    "**Prerequisites:** Tutorials 1-8 (understand RAG and the agent loop).\n",
    "Python basics. No prior knowledge of OpenTelemetry or Phoenix required.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Q[User Question] --> R[Retrieve]\n",
    "    R --> G[Generate Answer]\n",
    "    R -. span .-> T[(Trace Store)]\n",
    "    G -. span .-> T\n",
    "    T --> P[Phoenix UI]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57820e",
   "metadata": {},
   "source": [
    "## Why Does an LLM Workflow Need Tracing?\n",
    "\n",
    "### The Problem with Black-Box Pipelines\n",
    "\n",
    "After Tutorials 1-8 you have a working RAG pipeline and an agent loop.\n",
    "Both can answer policy questions. But when something goes wrong - a wrong\n",
    "answer, unexpected latency, or a retrieval miss - answering \"why?\" is hard\n",
    "without detailed records of each step.\n",
    "\n",
    "| Problem | Without tracing | With tracing |\n",
    "|---------|----------------|--------------|\n",
    "| Wrong answer | Re-read code; guess | Inspect the exact chunks the LLM received |\n",
    "| High latency | Add print statements | See per-step timing in a timeline |\n",
    "| Retrieval miss | Check embedding manually | See the query, top-k, and scores per call |\n",
    "| Agent loop bug | Add logging everywhere | Read Thought/Action/Observation for every step |\n",
    "\n",
    "### What Tracing Adds\n",
    "\n",
    "Tracing instruments your code so that each meaningful operation emits a\n",
    "structured record called a **span**. A span captures:\n",
    "\n",
    "- a name (for example 'retrieval' or 'generation')\n",
    "- a start timestamp and an end timestamp\n",
    "- a status (OK or ERROR)\n",
    "- arbitrary key-value **attributes** (for example query='leave policy', top_k=5)\n",
    "\n",
    "Spans from the same user request are linked into a **trace**, which looks like\n",
    "a nested timeline. A trace viewer such as Arize Phoenix shows you every step\n",
    "of every request in one place.\n",
    "\n",
    "```\n",
    "Trace for question: 'What is the international work limit?'\n",
    "  |-- retrieval  (12 ms)   query='international work limit'  result_count=5\n",
    "  |-- generation (340 ms)  model='gpt-4.1-mini'  chunks=5  answer_words=28\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d81e9",
   "metadata": {},
   "source": [
    "## What Is OpenTelemetry?\n",
    "\n",
    "**OpenTelemetry** (OTel) is an open standard for collecting observability\n",
    "signals (traces, metrics, logs) from any software system. It is vendor-neutral:\n",
    "the same instrumentation code can send data to Arize Phoenix, Jaeger, Grafana\n",
    "Tempo, or any OTLP-compatible backend.\n",
    "\n",
    "The three key concepts you need for this tutorial:\n",
    "\n",
    "| Concept | What it is | Example |\n",
    "|---------|-----------|--------|\n",
    "| TracerProvider | The factory that creates tracers; also holds the exporter | One per application |\n",
    "| Tracer | Creates spans; you call tracer.start_as_current_span() | One per module |\n",
    "| Span | A single timed operation with attributes | 'retrieval' span |\n",
    "\n",
    "**How data flows:**\n",
    "\n",
    "```\n",
    "Your code\n",
    "  -> tracer.start_as_current_span('retrieval')\n",
    "  -> span.set_attribute('retrieval.query', 'leave policy')\n",
    "  -> span ends\n",
    "  -> SpanProcessor sends the finished span to an Exporter\n",
    "  -> Exporter writes to: in-memory buffer  OR  Phoenix server  OR  cloud backend\n",
    "```\n",
    "\n",
    "**This tutorial uses two exporters:**\n",
    "\n",
    "1. `InMemorySpanExporter` - stores spans in a Python list; no server needed;\n",
    "   used for the offline demos in this notebook.\n",
    "2. Phoenix OTLP exporter - sends spans over HTTP to a running Phoenix server;\n",
    "   used in the Phoenix integration section at the end.\n",
    "\n",
    "**What is Arize Phoenix?**\n",
    "\n",
    "Arize Phoenix is an open-source LLM observability platform. It provides:\n",
    "- a local server that receives OpenTelemetry traces\n",
    "- a browser UI that shows traces, timelines, and span attributes\n",
    "- built-in evaluation tools for LLM output quality\n",
    "\n",
    "You will use it as a trace viewer after learning to capture spans manually.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Code[Your Python code] -->|OTel spans| SP[SpanProcessor]\n",
    "    SP -->|offline demo| Mem[InMemorySpanExporter]\n",
    "    SP -->|Phoenix mode| OTLP[OTLP HTTP Exporter]\n",
    "    OTLP --> PX[Phoenix Server :6006]\n",
    "    PX --> UI[Browser UI]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361138ec",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Load Environment\n",
    "\n",
    "The cell below installs dependencies and loads the same RAG pipeline used in\n",
    "Tutorials 1-8. It also imports the `tracing` module from `rag_tutorials` which\n",
    "provides the span-recording helpers you will use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents]\n",
    "     if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\", \"chromadb\", \"numpy\", \"pandas\", \"rank_bm25\",\n",
    "    \"sentence_transformers\", \"dotenv\", \"opentelemetry\",\n",
    "    \"arize\", \"openinference\",\n",
    "]\n",
    "PIP_NAME_MAP = {\n",
    "    \"rank_bm25\": \"rank-bm25\",\n",
    "    \"sentence_transformers\": \"sentence-transformers\",\n",
    "    \"dotenv\": \"python-dotenv\",\n",
    "    \"opentelemetry\": \"opentelemetry-sdk\",\n",
    "    \"arize\": \"arize-phoenix\",\n",
    "    \"openinference\": \"openinference-instrumentation-openai\",\n",
    "}\n",
    "\n",
    "def find_missing(packages):\n",
    "    importlib.invalidate_caches()\n",
    "    return [\n",
    "        pkg for pkg in packages\n",
    "        if importlib.util.find_spec(pkg.split(\".\")[0]) is None\n",
    "    ]\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing: {final_missing}\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.tracing import (\n",
    "    build_in_memory_tracer,\n",
    "    record_retrieval_span,\n",
    "    record_generation_span,\n",
    "    record_agent_step_span,\n",
    "    spans_to_dicts,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)\n",
    "chunks = semantic_chunk_documents(documents)\n",
    "dense_retriever, _ = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tracing_tutorial_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "print(\"Setup complete.\")\n",
    "print(f\"Documents loaded : {len(documents)}\")\n",
    "print(f\"Chunks created   : {len(chunks)}\")\n",
    "print(f\"Queries loaded   : {len(queries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa848e",
   "metadata": {},
   "source": [
    "## Part 1: Capturing Spans In Memory\n",
    "\n",
    "### What Does a Span Look Like?\n",
    "\n",
    "A span is a Python object with the following fields:\n",
    "\n",
    "| Field | Type | Meaning |\n",
    "|-------|------|---------|\n",
    "| `name` | str | Operation name, e.g. 'retrieval' |\n",
    "| `start_time` | int (nanoseconds) | When the operation started |\n",
    "| `end_time` | int (nanoseconds) | When the operation finished |\n",
    "| `status` | StatusCode | OK or ERROR |\n",
    "| `attributes` | dict | Key-value metadata, e.g. {'retrieval.query': 'leave policy'} |\n",
    "\n",
    "The `build_in_memory_tracer()` helper creates a fresh `TracerProvider` and\n",
    "`InMemorySpanExporter`. All spans started with the returned tracer are stored\n",
    "inside the exporter until you call `exporter.get_finished_spans()`.\n",
    "\n",
    "In the next cell you will create your first span manually to see the raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tracer and exporter backed by an in-memory buffer.\n",
    "# No server, no network calls - spans are stored as Python objects.\n",
    "\n",
    "tracer, exporter = build_in_memory_tracer()\n",
    "\n",
    "# Start a span, set some attributes, then end it.\n",
    "# The 'with' block defines the span's lifetime:\n",
    "# the span starts when the block is entered and ends when it exits.\n",
    "with tracer.start_as_current_span(\"my_first_span\") as span:\n",
    "    span.set_attribute(\"greeting\", \"hello world\")\n",
    "    span.set_attribute(\"step\", 1)\n",
    "    # Any code here runs 'inside' the span.\n",
    "    result = 2 + 2\n",
    "\n",
    "# The span is now finished. Read it back from the exporter.\n",
    "finished = exporter.get_finished_spans()\n",
    "print(f\"Number of finished spans : {len(finished)}\")\n",
    "\n",
    "span_obj = finished[0]\n",
    "print(f\"Name       : {span_obj.name}\")\n",
    "print(f\"Status     : {span_obj.status.status_code.name}\")\n",
    "print(f\"Attributes : {dict(span_obj.attributes)}\")\n",
    "\n",
    "duration_ns = span_obj.end_time - span_obj.start_time\n",
    "print(f\"Duration   : {duration_ns / 1_000_000:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f420d4d",
   "metadata": {},
   "source": [
    "### Converting Spans to a Display Table\n",
    "\n",
    "Raw span objects have nanosecond timestamps and OpenTelemetry status objects.\n",
    "The `spans_to_dicts()` helper converts a list of finished spans into plain\n",
    "Python dicts so you can display them with pandas.\n",
    "\n",
    "Each dict has these keys:\n",
    "\n",
    "- `name` - span name\n",
    "- `status` - 'UNSET' (healthy), 'OK', or 'ERROR'\n",
    "- `duration_ms` - wall-clock duration in milliseconds\n",
    "- `attributes` - dict of all set attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ac68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spans_to_dicts to convert spans to a readable format.\n",
    "\n",
    "# First create a few spans so the table has multiple rows.\n",
    "tracer2, exporter2 = build_in_memory_tracer()\n",
    "\n",
    "import time\n",
    "\n",
    "with tracer2.start_as_current_span(\"step_one\") as s:\n",
    "    s.set_attribute(\"info\", \"preprocessing\")\n",
    "    time.sleep(0.005)   # simulate 5 ms of work\n",
    "\n",
    "with tracer2.start_as_current_span(\"step_two\") as s:\n",
    "    s.set_attribute(\"info\", \"retrieval\")\n",
    "    time.sleep(0.012)   # simulate 12 ms of work\n",
    "\n",
    "with tracer2.start_as_current_span(\"step_three\") as s:\n",
    "    s.set_attribute(\"info\", \"generation\")\n",
    "    time.sleep(0.025)   # simulate 25 ms of work\n",
    "\n",
    "span_dicts = spans_to_dicts(exporter2.get_finished_spans())\n",
    "\n",
    "# Show as a summary table (without the nested attributes column)\n",
    "df_spans = pd.DataFrame([\n",
    "    {\"name\": d[\"name\"], \"status\": d[\"status\"], \"duration_ms\": d[\"duration_ms\"]}\n",
    "    for d in span_dicts\n",
    "])\n",
    "print(df_spans.to_string(index=False))\n",
    "\n",
    "# Show the attributes for each span\n",
    "print(\"\\nAttributes per span:\")\n",
    "for d in span_dicts:\n",
    "    print(f\"  {d['name']}: {d['attributes']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c73710",
   "metadata": {},
   "source": [
    "## Part 2: Tracing a Retrieval Step\n",
    "\n",
    "The `record_retrieval_span()` helper wraps a retriever call in a span so that\n",
    "every call records:\n",
    "\n",
    "| Attribute | What it captures |\n",
    "|-----------|------------------|\n",
    "| `retrieval.query` | The query string passed to the retriever |\n",
    "| `retrieval.top_k` | Maximum results requested |\n",
    "| `retrieval.result_count` | Actual results returned |\n",
    "| `retrieval.top_score` | Cosine similarity of the best match (if any) |\n",
    "\n",
    "**Why is this useful?**\n",
    "\n",
    "If the agent is getting a wrong answer, you can check `retrieval.top_score` to\n",
    "see whether the retrieval was confident. A low top score means the vector index\n",
    "did not find a good match and the answer is likely to be wrong.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "You call `record_retrieval_span()` *after* you have the results back from the\n",
    "retriever. The function starts a span, sets the attributes, and ends the span\n",
    "all inside a single `with` block, so the duration captures the full retrieval\n",
    "latency when you put the retriever call inside the block instead of before it.\n",
    "\n",
    "The example below shows both patterns:\n",
    "1. Record attributes from a finished retrieval (simpler)\n",
    "2. Run the retrieval inside the span (records exact latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: record attributes from results you already have\n",
    "\n",
    "query = \"What is the maximum number of days an employee can work internationally?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# Run the retriever\n",
    "results = dense_retriever(query, top_k=TOP_K)\n",
    "\n",
    "# Create a fresh tracer for this demo\n",
    "tracer_r, exporter_r = build_in_memory_tracer()\n",
    "\n",
    "# Record a span that describes this retrieval call\n",
    "record_retrieval_span(tracer_r, query=query, results=results, top_k=TOP_K)\n",
    "\n",
    "span_data = spans_to_dicts(exporter_r.get_finished_spans())[0]\n",
    "print(\"Retrieval span attributes:\")\n",
    "for key, value in span_data[\"attributes\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nDuration: {span_data['duration_ms']} ms  (recording overhead only)\")\n",
    "print()\n",
    "\n",
    "# Show the top retrieved chunks for inspection\n",
    "print(\"Top chunks retrieved:\")\n",
    "df_results = pd.DataFrame([\n",
    "    {\"rank\": i+1, \"chunk_id\": r.chunk_id, \"score\": round(r.score, 4),\n",
    "     \"preview\": r.text[:80] + \"...\"}\n",
    "    for i, r in enumerate(results)\n",
    "])\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2025e1",
   "metadata": {},
   "source": [
    "## Part 3: Tracing an Answer-Generation Step\n",
    "\n",
    "The `record_generation_span()` helper records the LLM call that turns\n",
    "retrieved chunks into a final answer. It captures:\n",
    "\n",
    "| Attribute | What it captures |\n",
    "|-----------|------------------|\n",
    "| `generation.question` | The user question |\n",
    "| `generation.model` | Chat model name |\n",
    "| `generation.context_chunk_count` | Number of chunks in the prompt |\n",
    "| `generation.answer_word_count` | Number of words in the answer |\n",
    "\n",
    "**Note:** Recording the full question and answer text inside a span attribute\n",
    "is useful for debugging but should be done selectively in production because\n",
    "long strings increase storage and may contain sensitive content.\n",
    "\n",
    "In the cell below we run a full retrieval-then-generation pipeline and trace\n",
    "both steps so you can see a two-span trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694878c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace a full retrieval -> generation pipeline.\n",
    "\n",
    "tracer_pipe, exporter_pipe = build_in_memory_tracer()\n",
    "\n",
    "question = \"What is the policy for working remotely from another country?\"\n",
    "\n",
    "# Step 1: Retrieve with tracing\n",
    "retrieved = dense_retriever(question, top_k=5)\n",
    "record_retrieval_span(tracer_pipe, query=question, results=retrieved, top_k=5)\n",
    "\n",
    "# Step 2: Generate answer with tracing\n",
    "context_chunks = [r.text for r in retrieved]\n",
    "answer = answer_with_context(question, context_chunks, model=chat_model)\n",
    "record_generation_span(\n",
    "    tracer_pipe,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    model=chat_model,\n",
    "    context_chunk_count=len(context_chunks),\n",
    ")\n",
    "\n",
    "# Display the two-span trace as a table\n",
    "trace_dicts = spans_to_dicts(exporter_pipe.get_finished_spans())\n",
    "df_trace = pd.DataFrame([\n",
    "    {\n",
    "        \"span\": d[\"name\"],\n",
    "        \"status\": d[\"status\"],\n",
    "        \"duration_ms\": d[\"duration_ms\"],\n",
    "        \"key_attribute\": next(iter(d[\"attributes\"].items()), (\"\", \"\"))[1],\n",
    "    }\n",
    "    for d in trace_dicts\n",
    "])\n",
    "print(\"Two-span trace summary:\")\n",
    "print(df_trace.to_string(index=False))\n",
    "\n",
    "print(\"\\nFull attribute details:\")\n",
    "for d in trace_dicts:\n",
    "    print(f\"\\n  [{d['name']}]\")\n",
    "    for k, v in d[\"attributes\"].items():\n",
    "        v_display = (str(v)[:80] + \"...\") if len(str(v)) > 80 else v\n",
    "        print(f\"    {k}: {v_display}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8076f",
   "metadata": {},
   "source": [
    "## Part 4: Tracing Agent Steps\n",
    "\n",
    "In Tutorial 6 the ReAct agent produces a series of Thought-Action-Observation\n",
    "cycles. Without tracing, you can only see these by printing them to the\n",
    "terminal. With tracing, each step is a span you can query and filter.\n",
    "\n",
    "The `record_agent_step_span()` helper records each cycle as a span named\n",
    "`agent_step` with these attributes:\n",
    "\n",
    "| Attribute | What it captures |\n",
    "|-----------|------------------|\n",
    "| `agent.step_number` | Step index within the loop (1-based) |\n",
    "| `agent.thought` | The agent's reasoning text |\n",
    "| `agent.action` | Tool name called ('retrieve' or 'finish') |\n",
    "| `agent.action_input` | Input string passed to the tool |\n",
    "| `agent.observation_length` | Length of the tool's output in characters |\n",
    "\n",
    "The cell below builds a **traced agent loop**: a wrapper around\n",
    "`run_react_loop` that records a span for each step after the loop completes.\n",
    "\n",
    "**Observation:** After running, you can query the trace to answer questions\n",
    "like 'how many steps did the agent take?' or 'what did the agent search for?'\n",
    "without re-reading the agent's terminal output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_tutorials.agent_loop import run_react_loop\n",
    "\n",
    "\n",
    "def traced_agent_run(question: str, tracer, top_k: int = 3) -> dict:\n",
    "    \"\"\"Run the ReAct agent and record each step as an agent_step span.\n",
    "\n",
    "    Args:\n",
    "        question: User question to answer.\n",
    "        tracer: Active OpenTelemetry tracer.\n",
    "        top_k: Number of chunks per retrieval call.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'answer', 'step_count', and 'steps'.\n",
    "    \"\"\"\n",
    "    def retrieve_tool(query: str) -> str:\n",
    "        results = dense_retriever(query, top_k=top_k)\n",
    "        if not results:\n",
    "            return \"No relevant chunks found.\"\n",
    "        parts = [f\"Chunk {i+1} [{r.chunk_id}]: {r.text}\" for i, r in enumerate(results)]\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "    tools = {\"retrieve\": retrieve_tool}\n",
    "    result = run_react_loop(question=question, tools=tools, model=chat_model, max_steps=5)\n",
    "\n",
    "    for i, step in enumerate(result.steps, start=1):\n",
    "        record_agent_step_span(\n",
    "            tracer,\n",
    "            step_number=i,\n",
    "            thought=step.thought,\n",
    "            action=step.action,\n",
    "            action_input=step.action_input,\n",
    "            observation=step.observation,\n",
    "        )\n",
    "\n",
    "    return {\"answer\": result.answer, \"step_count\": len(result.steps), \"steps\": result.steps}\n",
    "\n",
    "\n",
    "tracer_agent, exporter_agent = build_in_memory_tracer()\n",
    "\n",
    "agent_question = \"What happens if an employee works internationally beyond the allowed limit?\"\n",
    "agent_run = traced_agent_run(agent_question, tracer_agent)\n",
    "\n",
    "print(f\"Question   : {agent_question}\")\n",
    "print(f\"Steps taken: {agent_run['step_count']}\")\n",
    "print(f\"Answer     : {agent_run['answer']}\")\n",
    "\n",
    "print(\"\\nAgent step spans:\")\n",
    "agent_span_dicts = spans_to_dicts(exporter_agent.get_finished_spans())\n",
    "for d in agent_span_dicts:\n",
    "    attrs = d[\"attributes\"]\n",
    "    print(\n",
    "        f\"  step {attrs.get('agent.step_number')}\"\n",
    "        f\"  action={attrs.get('agent.action')!r}\"\n",
    "        f\"  obs_len={attrs.get('agent.observation_length')}\"\n",
    "        f\"  duration={d['duration_ms']} ms\"\n",
    "    )\n",
    "print()\n",
    "print(\"Thoughts recorded:\")\n",
    "for d in agent_span_dicts:\n",
    "    print(f\"  Step {d['attributes']['agent.step_number']}: {d['attributes']['agent.thought'][:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0787e",
   "metadata": {},
   "source": [
    "## Part 5: Connecting to Arize Phoenix\n",
    "\n",
    "The in-memory approach you used above captures spans in Python objects.\n",
    "This is useful for quick inspection but there is no persistent storage and\n",
    "no visual timeline. Arize Phoenix solves both problems.\n",
    "\n",
    "### What Phoenix Provides\n",
    "\n",
    "When you send traces to Phoenix:\n",
    "\n",
    "1. Every span is stored in Phoenix's local database.\n",
    "2. Phoenix groups spans from the same request into a trace timeline.\n",
    "3. You can filter traces by project, by span name, by attribute value.\n",
    "4. You can view the nested span tree for each request in a timeline diagram.\n",
    "\n",
    "### How to Start a Local Phoenix Server\n",
    "\n",
    "Phoenix can run as a local server that receives spans via OTLP/HTTP on port\n",
    "6006. There are two ways to start it:\n",
    "\n",
    "**Option A: In-notebook launch (simplest)**\n",
    "\n",
    "```python\n",
    "import phoenix as px\n",
    "session = px.launch_app()      # starts the Phoenix server in the background\n",
    "print(session.url)             # open this URL in your browser\n",
    "```\n",
    "\n",
    "**Option B: Command-line launch**\n",
    "\n",
    "```bash\n",
    "python -m phoenix.server.main serve\n",
    "# then open http://localhost:6006 in your browser\n",
    "```\n",
    "\n",
    "### How to Register the Phoenix Tracer\n",
    "\n",
    "After starting the server, register Phoenix as the global OTLP destination:\n",
    "\n",
    "```python\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"rag_tutorial\",\n",
    "    endpoint=\"http://localhost:6006/v1/traces\",  # Phoenix OTLP endpoint\n",
    ")\n",
    "```\n",
    "\n",
    "After calling `register()`, the global OpenTelemetry tracer provider is set to\n",
    "Phoenix. Any span you create from this point on is automatically sent to the\n",
    "Phoenix server.\n",
    "\n",
    "### How to Auto-Instrument OpenAI\n",
    "\n",
    "OpenInference provides auto-instrumentation for the OpenAI Python SDK:\n",
    "\n",
    "```python\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "```\n",
    "\n",
    "After this call, every `client.chat.completions.create()` and\n",
    "`client.responses.create()` call automatically produces a span with the\n",
    "model name, prompt tokens, completion tokens, and latency - no manual\n",
    "span recording required.\n",
    "\n",
    "The next cells set up Phoenix and run the traced pipeline with auto-instrumentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41caa3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell starts a local Phoenix server and registers it as the\n",
    "# global OpenTelemetry tracer provider.\n",
    "#\n",
    "# If Phoenix is not installed or the server is already running,\n",
    "# the cell prints a clear error and continues safely.\n",
    "\n",
    "phoenix_available = False\n",
    "phoenix_session = None\n",
    "phoenix_tracer_provider = None\n",
    "\n",
    "try:\n",
    "    import phoenix as px\n",
    "    from phoenix.otel import register\n",
    "\n",
    "    phoenix_session = px.launch_app()\n",
    "    phoenix_url = phoenix_session.url\n",
    "\n",
    "    phoenix_tracer_provider = register(\n",
    "        project_name=\"rag_tutorial\",\n",
    "        endpoint=f\"{phoenix_url}v1/traces\",\n",
    "    )\n",
    "\n",
    "    phoenix_available = True\n",
    "    print(f\"Phoenix server started: {phoenix_url}\")\n",
    "    print(\"Open the URL above in your browser to view traces.\")\n",
    "    print(\"Project: rag_tutorial\")\n",
    "\n",
    "except Exception as exc:\n",
    "    print(f\"Phoenix not available: {exc}\")\n",
    "    print(\"The in-memory tracing cells above still work without Phoenix.\")\n",
    "    print(\"To enable Phoenix: uv sync (installs arize-phoenix from pyproject.toml)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-instrument OpenAI if Phoenix is running.\n",
    "# After this call, every OpenAI API call is automatically traced.\n",
    "\n",
    "auto_instrumented = False\n",
    "\n",
    "if phoenix_available and phoenix_tracer_provider is not None:\n",
    "    try:\n",
    "        from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "        OpenAIInstrumentor().instrument(tracer_provider=phoenix_tracer_provider)\n",
    "        auto_instrumented = True\n",
    "        print(\"OpenAI auto-instrumentation active.\")\n",
    "        print(\"All embedding and chat calls will be traced automatically.\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Auto-instrumentation unavailable: {exc}\")\n",
    "else:\n",
    "    print(\"Skipping auto-instrumentation (Phoenix not running).\")\n",
    "    print(\"Using manual record_*_span() helpers instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d27ff",
   "metadata": {},
   "source": [
    "### Running the Traced Pipeline Against Phoenix\n",
    "\n",
    "The cell below runs the same retrieval and generation steps as Part 3 but\n",
    "with the Phoenix tracer provider active. After running:\n",
    "\n",
    "1. Open the Phoenix URL printed above.\n",
    "2. Select the 'rag_tutorial' project.\n",
    "3. Each row in the traces list represents one pipeline run.\n",
    "4. Click a row to see the span tree and every attribute.\n",
    "\n",
    "**What to look for in Phoenix:**\n",
    "\n",
    "- The 'retrieval' span shows the query and top score.\n",
    "- The 'generation' span shows the model and word count.\n",
    "- If auto-instrumentation is active, you will also see child spans for\n",
    "  the embedding API call and the chat completion call, with token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ae757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline with Phoenix tracing if available,\n",
    "# otherwise fall back to in-memory tracing.\n",
    "\n",
    "from opentelemetry import trace as otel_trace\n",
    "from rag_tutorials.tracing import TRACER_NAME\n",
    "\n",
    "if phoenix_available and phoenix_tracer_provider is not None:\n",
    "    # Use the Phoenix provider that was registered by phoenix.otel.register()\n",
    "    phoenix_tracer = phoenix_tracer_provider.get_tracer(TRACER_NAME)\n",
    "    exporter_px = None\n",
    "    print(\"Using Phoenix tracer (spans sent to Phoenix server)\")\n",
    "else:\n",
    "    # Fall back to in-memory\n",
    "    phoenix_tracer, exporter_px = build_in_memory_tracer()\n",
    "    print(\"Using in-memory tracer (Phoenix not running)\")\n",
    "\n",
    "phoenix_question = \"What are the requirements for a Global Mobility case?\"\n",
    "\n",
    "# Retrieve\n",
    "px_results = dense_retriever(phoenix_question, top_k=5)\n",
    "record_retrieval_span(phoenix_tracer, query=phoenix_question, results=px_results, top_k=5)\n",
    "\n",
    "# Generate\n",
    "px_context = [r.text for r in px_results]\n",
    "px_answer = answer_with_context(phoenix_question, px_context, model=chat_model)\n",
    "record_generation_span(\n",
    "    phoenix_tracer,\n",
    "    question=phoenix_question,\n",
    "    answer=px_answer,\n",
    "    model=chat_model,\n",
    "    context_chunk_count=len(px_context),\n",
    ")\n",
    "\n",
    "if exporter_px is not None:\n",
    "    # In-memory fallback: print the trace locally\n",
    "    px_span_dicts = spans_to_dicts(exporter_px.get_finished_spans())\n",
    "    print(\"\\nIn-memory trace (Phoenix not running):\")\n",
    "    for d in px_span_dicts:\n",
    "        print(f\"  [{d['name']}] {d['duration_ms']} ms   {d['attributes']}\")\n",
    "\n",
    "print(f\"\\nAnswer: {px_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9153e649",
   "metadata": {},
   "source": [
    "## Part 6: Tracing Multiple Questions (Benchmark Mode)\n",
    "\n",
    "The cells above traced individual queries one at a time. In a real system you\n",
    "want to trace all evaluation queries so you can spot patterns:\n",
    "\n",
    "- Which queries produce low top scores (likely retrieval misses)?\n",
    "- Which queries generate the longest answers?\n",
    "- Which queries take the most time?\n",
    "\n",
    "The cell below runs five queries from the shared query set, records a span\n",
    "for each step, and summarises the trace table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace five queries and summarise the results.\n",
    "\n",
    "tracer_bench, exporter_bench = build_in_memory_tracer()\n",
    "\n",
    "eval_queries = queries[:5]\n",
    "\n",
    "for q in eval_queries:\n",
    "    bench_results = dense_retriever(q.question, top_k=5)\n",
    "    record_retrieval_span(\n",
    "        tracer_bench, query=q.question, results=bench_results, top_k=5\n",
    "    )\n",
    "\n",
    "    bench_context = [r.text for r in bench_results]\n",
    "    bench_answer = answer_with_context(q.question, bench_context, model=chat_model)\n",
    "    record_generation_span(\n",
    "        tracer_bench,\n",
    "        question=q.question,\n",
    "        answer=bench_answer,\n",
    "        model=chat_model,\n",
    "        context_chunk_count=len(bench_context),\n",
    "    )\n",
    "\n",
    "all_span_dicts = spans_to_dicts(exporter_bench.get_finished_spans())\n",
    "\n",
    "retrieval_spans = [d for d in all_span_dicts if d[\"name\"] == \"retrieval\"]\n",
    "generation_spans = [d for d in all_span_dicts if d[\"name\"] == \"generation\"]\n",
    "\n",
    "print(f\"Total spans captured : {len(all_span_dicts)}\")\n",
    "print(f\"  retrieval spans    : {len(retrieval_spans)}\")\n",
    "print(f\"  generation spans   : {len(generation_spans)}\")\n",
    "\n",
    "print(\"\\nRetrieval span summary:\")\n",
    "df_ret = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": d[\"attributes\"].get(\"retrieval.query\", \"\")[:50] + \"...\",\n",
    "        \"result_count\": d[\"attributes\"].get(\"retrieval.result_count\"),\n",
    "        \"top_score\": round(d[\"attributes\"].get(\"retrieval.top_score\", 0), 4),\n",
    "        \"duration_ms\": d[\"duration_ms\"],\n",
    "    }\n",
    "    for d in retrieval_spans\n",
    "])\n",
    "print(df_ret.to_string(index=False))\n",
    "\n",
    "print(\"\\nGeneration span summary:\")\n",
    "df_gen = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": d[\"attributes\"].get(\"generation.model\"),\n",
    "        \"chunks\": d[\"attributes\"].get(\"generation.context_chunk_count\"),\n",
    "        \"answer_words\": d[\"attributes\"].get(\"generation.answer_word_count\"),\n",
    "        \"duration_ms\": d[\"duration_ms\"],\n",
    "    }\n",
    "    for d in generation_spans\n",
    "])\n",
    "print(df_gen.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edc44a",
   "metadata": {},
   "source": [
    "## Part 7: Visualising Trace Data in the Notebook\n",
    "\n",
    "When Phoenix is not available, you can still visualise trace data using\n",
    "matplotlib. The chart below shows the per-step latency breakdown for the\n",
    "five benchmark queries so you can identify which operations are slowest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c203129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a side-by-side bar chart of retrieval vs generation latency\n",
    "retrieval_times = [d[\"duration_ms\"] for d in retrieval_spans]\n",
    "generation_times = [d[\"duration_ms\"] for d in generation_spans]\n",
    "query_labels = [f\"Q{i+1}\" for i in range(len(retrieval_spans))]\n",
    "\n",
    "x = range(len(query_labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars_ret = ax.bar([xi - width/2 for xi in x], retrieval_times, width, label=\"retrieval\", color=\"steelblue\")\n",
    "bars_gen = ax.bar([xi + width/2 for xi in x], generation_times, width, label=\"generation\", color=\"coral\")\n",
    "\n",
    "ax.set_xlabel(\"Query\")\n",
    "ax.set_ylabel(\"Duration (ms)\")\n",
    "ax.set_title(\"Per-query latency: retrieval vs generation (from trace spans)\")\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(query_labels)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_ret = sum(retrieval_times)\n",
    "total_gen = sum(generation_times)\n",
    "pct_ret = 100 * total_ret / (total_ret + total_gen) if (total_ret + total_gen) > 0 else 0\n",
    "pct_gen = 100 * total_gen / (total_ret + total_gen) if (total_ret + total_gen) > 0 else 0\n",
    "print(f\"Average retrieval latency : {sum(retrieval_times)/len(retrieval_times):.1f} ms ({pct_ret:.0f}% of total)\")\n",
    "print(f\"Average generation latency: {sum(generation_times)/len(generation_times):.1f} ms ({pct_gen:.0f}% of total)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5628eb",
   "metadata": {},
   "source": [
    "## Part 8: Tracing Error and Edge-Case Paths\n",
    "\n",
    "A span can have status `OK` or `ERROR`. Recording errors in spans lets you\n",
    "filter for failed requests in Phoenix and inspect exactly which attributes\n",
    "were set at the time of the failure.\n",
    "\n",
    "The cell below demonstrates wrapping a failing operation in a span that\n",
    "records the error status.\n",
    "\n",
    "**How to set an error status on a span:**\n",
    "\n",
    "```python\n",
    "from opentelemetry.trace import StatusCode\n",
    "\n",
    "with tracer.start_as_current_span(\"my_operation\") as span:\n",
    "    try:\n",
    "        result = risky_operation()\n",
    "        span.set_status(StatusCode.OK)\n",
    "    except Exception as exc:\n",
    "        span.set_status(StatusCode.ERROR, description=str(exc))\n",
    "        span.record_exception(exc)\n",
    "        raise\n",
    "```\n",
    "\n",
    "The `record_exception()` call stores the exception type, message, and\n",
    "stack trace as span events so you can read them in Phoenix.\n",
    "\n",
    "The cell below shows a retrieval span on an empty query (a common edge case)\n",
    "and confirms the span still has UNSET status (no error was raised - the\n",
    "retriever returns an empty list gracefully).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0090b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.trace import StatusCode\n",
    "\n",
    "tracer_err, exporter_err = build_in_memory_tracer()\n",
    "\n",
    "# Edge case 1: empty query -> retriever returns empty list; span is still OK\n",
    "empty_results = dense_retriever(\"\", top_k=3)\n",
    "record_retrieval_span(tracer_err, query=\"\", results=empty_results, top_k=3)\n",
    "\n",
    "# Edge case 2: manually record a simulated error span\n",
    "with tracer_err.start_as_current_span(\"retrieval_error_example\") as err_span:\n",
    "    try:\n",
    "        raise ValueError(\"Simulated retriever failure: index unavailable\")\n",
    "    except ValueError as exc:\n",
    "        err_span.set_status(StatusCode.ERROR, description=str(exc))\n",
    "        err_span.record_exception(exc)\n",
    "\n",
    "error_spans = spans_to_dicts(exporter_err.get_finished_spans())\n",
    "print(\"Error-path span statuses:\")\n",
    "for d in error_spans:\n",
    "    print(f\"  name={d['name']!r}  status={d['status']}  attrs={d['attributes']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495425f2",
   "metadata": {},
   "source": [
    "## Learning Checkpoint: Tracing with Arize Phoenix\n",
    "\n",
    "### What Works\n",
    "\n",
    "- Spans capture structured metadata about each pipeline step without changing\n",
    "  the step's logic. The span is an observer, not a participant.\n",
    "- `build_in_memory_tracer()` lets you capture and inspect spans entirely in\n",
    "  Python without a server, which makes tracing easy to add to any tutorial.\n",
    "- `spans_to_dicts()` converts spans to a table-friendly format so you can\n",
    "  query trace data with pandas.\n",
    "- `record_retrieval_span()`, `record_generation_span()`, and\n",
    "  `record_agent_step_span()` are thin wrappers that cover the most important\n",
    "  steps in the RAG and agent workflow without duplicating business logic.\n",
    "- When Phoenix is running, the same spans appear in a browser timeline with\n",
    "  no code changes beyond calling `register()` at startup.\n",
    "\n",
    "### What Does Not Work Well\n",
    "\n",
    "- Manual span recording requires you to remember to call the helper after\n",
    "  every significant step. Auto-instrumentation (via OpenInference) removes\n",
    "  this burden for OpenAI API calls but not for custom retrieval code.\n",
    "- The in-memory exporter discards spans when the Python process ends. For\n",
    "  persistent trace storage, a running Phoenix server (or a cloud backend)\n",
    "  is required.\n",
    "- Span attributes are limited to primitive types (strings, numbers, booleans).\n",
    "  Complex objects such as chunk lists must be summarised (e.g. by recording\n",
    "  only the count and the top score, not the full text).\n",
    "\n",
    "### How to Extend This in Production\n",
    "\n",
    "- Replace `build_in_memory_tracer()` with `phoenix.otel.register()` at\n",
    "  application startup to send all traces to a persistent Phoenix server.\n",
    "- Use `openinference.instrumentation.openai.OpenAIInstrumentor` to automatically\n",
    "  capture token usage and prompt/completion text for every LLM call.\n",
    "- Add span nesting: start a parent 'pipeline' span that wraps both the\n",
    "  retrieval and generation child spans so they appear as one tree in Phoenix.\n",
    "- Use Phoenix's evaluation tools to run automated LLM-as-judge quality checks\n",
    "  on traced answers and flag low-quality responses.\n"
   ]
  }
 ]
}