{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30aa400a",
   "metadata": {},
   "source": [
    "# Tutorial 3 \u2014 Reranking (Two-Stage Retrieval)\n",
    "\n",
    "## Where You Are in the Learning Journey\n",
    "\n",
    "```\n",
    " Tutorial 1 --> Tutorial 2 --> Tutorial 3 --> Tutorial 4 --> Tutorial 5\n",
    " Basic RAG      Semantic       YOU ARE        Add Keyword    Benchmark\n",
    " (baseline)     Chunking       HERE           Search         All Four\n",
    "```\n",
    "\n",
    "**What changed from Tutorial 2:** one new stage added after retrieval \u2014 the **reranker**.\n",
    "Chunking, embeddings, and the vector store are identical to Tutorial 2.\n",
    "\n",
    "**What you will learn in this tutorial:**\n",
    "- What a reranker is and how it differs from a vector search\n",
    "- What 'bi-encoder' and 'cross-encoder' mean and why the difference matters\n",
    "- How two-stage retrieval works: retrieve a wide candidate set, then rerank to find the best\n",
    "- Why ranking position matters for LLM answer quality\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Query] --> B[Dense Retrieve top-10]\n",
    "    B --> C[Cross-Encoder Rerank]\n",
    "    C --> D[Top-5 for LLM]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e698583",
   "metadata": {},
   "source": [
    "## Learning Checkpoint: What Reranking Solves and What Remains\n",
    "\n",
    "### What Works Better in Tutorial 3\n",
    "\n",
    "- Candidate chunks are re-ordered with stronger query awareness \u2014 the reranker reads\n",
    "  the query and each chunk *together* and produces a relevance score that is\n",
    "  more accurate than cosine similarity alone.\n",
    "- The most relevant context is more likely to appear at rank 1, which the LLM\n",
    "  reads first (position bias: LLMs pay more attention to early context).\n",
    "\n",
    "### What Still Does Not Work Well\n",
    "\n",
    "- **Dense retrieval still dominates stage 1.** If a chunk is not in the first-stage\n",
    "  top-k (e.g. because the exact keywords are absent), the reranker never sees it.\n",
    "- **Exact-term queries** like 'Form A-12' are weakly handled by embedding similarity;\n",
    "  no reranker can recover a chunk that was never retrieved.\n",
    "\n",
    "### Why Move to Tutorial 4?\n",
    "\n",
    "Tutorial 4 adds **keyword (BM25) retrieval** alongside dense retrieval.  BM25 handles\n",
    "exact token matches that embeddings miss, giving the pipeline two complementary signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-5) Setup, load handbook text, chunk, embed, index\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\"openai\", \"chromadb\", \"numpy\", \"pandas\", \"rank_bm25\", \"sentence_transformers\", \"dotenv\"]\n",
    "PIP_NAME_MAP = {\"rank_bm25\": \"rank-bm25\", \"sentence_transformers\": \"sentence-transformers\", \"dotenv\": \"python-dotenv\"}\n",
    "\n",
    "def find_missing(packages: list[str]) -> list[str]:\n",
    "    importlib.invalidate_caches()\n",
    "    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    print(\"Installing into current kernel with pip:\", pip_targets)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing in current kernel: {final_missing}\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.reranking import LocalCrossEncoderReranker\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)\n",
    "chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "dense_retriever, _ = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial3_dense_semantic\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "reranker = LocalCrossEncoderReranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1ff05",
   "metadata": {},
   "source": [
    "### Key Concepts: What Is a Reranker and How Does It Work?\n",
    "\n",
    "#### Why Does Ranking Position Matter?\n",
    "\n",
    "When the LLM receives retrieved chunks, it reads them in order.\n",
    "Research consistently shows that information at the **beginning** of the context\n",
    "has more influence on the answer than information buried at rank 4 or 5.\n",
    "\n",
    "```\n",
    "Context sent to LLM:\n",
    "  [rank 1]  remote work policy \u2014 full section       <- LLM pays most attention here\n",
    "  [rank 2]  leave entitlement summary\n",
    "  [rank 3]  manager approval process\n",
    "  [rank 4]  tax implications \u2014 working abroad       <- LLM may under-weight this\n",
    "  [rank 5]  HR contact information\n",
    "\n",
    "If rank 4 is actually the most useful chunk, the LLM might produce a weaker answer.\n",
    "A reranker re-orders so the most useful chunk moves to rank 1.\n",
    "```\n",
    "\n",
    "#### What Is a Bi-Encoder? (Dense Retrieval = Stage 1)\n",
    "\n",
    "A **bi-encoder** is a model that converts text to a vector *independently*.\n",
    "'Bi' means two separate encodings \u2014 one for the query, one for each chunk.\n",
    "\n",
    "```\n",
    "Query:  \"remote work policy\"     -> Encoder -> query_vec  = [0.85, 0.15, 0.45, ...]\n",
    "Chunk:  \"Employees may work...\"  -> Encoder -> chunk_vec  = [0.80, 0.20, 0.50, ...]\n",
    "\n",
    "Similarity = cosine(query_vec, chunk_vec) = 0.97\n",
    "\n",
    "Key point: the encoder never sees query and chunk AT THE SAME TIME.\n",
    "This means it can pre-compute and store all chunk vectors in advance.\n",
    "```\n",
    "\n",
    "**Advantage:** Chunk vectors are computed once during indexing and reused for every query.\n",
    "Similarity lookup is fast \u2014 milliseconds even for thousands of chunks.\n",
    "\n",
    "**Limitation:** Because the query and chunk are encoded independently, the model\n",
    "cannot see how words in the query interact with words in the chunk.  Some\n",
    "relevance signals are missed.\n",
    "\n",
    "#### What Is a Cross-Encoder? (Reranking = Stage 2)\n",
    "\n",
    "A **cross-encoder** reads the query and a single chunk *together* as one combined input,\n",
    "then outputs a single relevance score.\n",
    "\n",
    "```\n",
    "Input:  [query] + [SEP] + [chunk]  ->  Cross-Encoder  ->  relevance score: 0.94\n",
    "\n",
    "Example:\n",
    "  Input:  \"What is the remote work policy? [SEP] Employees may work remotely...\"\n",
    "  Output: 0.94  (high relevance)\n",
    "\n",
    "  Input:  \"What is the remote work policy? [SEP] Annual leave entitlement is...\"\n",
    "  Output: 0.12  (low relevance)\n",
    "```\n",
    "\n",
    "**Advantage:** Sees the full interaction between query tokens and chunk tokens.\n",
    "Much more accurate relevance scores than cosine similarity.\n",
    "\n",
    "**Limitation:** Cannot pre-compute scores \u2014 must run the full model for every\n",
    "(query, chunk) pair at query time.  Too slow to run on all N chunks.\n",
    "That is why we use it only on the top-10 candidates from stage 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t3_vector_ref",
   "metadata": {},
   "source": [
    "### Two-Stage Retrieval: Why Not Just Use the Cross-Encoder Directly?\n",
    "\n",
    "A natural question: if the cross-encoder is more accurate, why use dense retrieval at all?\n",
    "\n",
    "**The answer is speed.**\n",
    "\n",
    "| | Bi-encoder (dense, Stage 1) | Cross-encoder (Stage 2) |\n",
    "|---|---|---|\n",
    "| **How it works** | Embeds query and chunk *independently*; compares vectors | Reads `(query, chunk)` as one input; models their interaction |\n",
    "| **Vectors pre-computed?** | Yes \u2014 chunk vectors stored in advance | No \u2014 must run every pair at query time |\n",
    "| **Speed** | Milliseconds for thousands of chunks | Seconds if run on thousands of chunks |\n",
    "| **Accuracy** | Good but misses subtle word interactions | Better \u2014 sees query and chunk together |\n",
    "| **Role in pipeline** | Fast first-pass: retrieve top-10 candidates | Accurate second-pass: reorder those 10 candidates |\n",
    "\n",
    "**Stage 1 \u2014 Dense retrieval (same cosine similarity as Tutorial 1)**  \n",
    "Query and every chunk are embedded into vectors; top-10 chunks by cosine score are selected.\n",
    "This is fast because chunk vectors were pre-computed and stored in Chroma.\n",
    "\n",
    "**Stage 2 \u2014 Cross-encoder reranking (new in this tutorial)**  \n",
    "The cross-encoder model reads each `(query, chunk)` pair and outputs a relevance score.\n",
    "It can see word-level interactions (e.g., the word \"cannot\" near \"work abroad\" matters).\n",
    "We only run it on 10 candidates \u2014 small enough to be fast, accurate enough to reorder well.\n",
    "\n",
    "**Why top-10 \u2192 top-5?**  \n",
    "We ask Stage 1 for 10 candidates (more recall), rerank all 10, then send only the top-5 to the LLM.\n",
    "This balances cost (LLM input tokens) with accuracy (best context available).\n",
    "\n",
    "See **Tutorial 1, cells 8\u201310** for the cosine similarity basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1005d",
   "metadata": {},
   "source": [
    "### Nearest-Neighbor Search in Stage 1 \u2014 How Candidates Are Shortlisted\n",
    "\n",
    "Before the reranker sees anything, **dense retrieval** runs nearest-neighbor search\n",
    "to produce a shortlist of candidates.  Here is what that looks like:\n",
    "\n",
    "```\n",
    "Stage 1 \u2014 Dense nearest-neighbor (top-k=10)\n",
    "\n",
    " rank  score   chunk\n",
    " \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "   1   0.91    remote work policy \u2014 full section\n",
    "   2   0.87    international transfer rules\n",
    "   3   0.80    home-office equipment allowance\n",
    "   4   0.76    tax implications \u2014 working abroad\n",
    "   5   0.70    remote work \u2014 manager approval process\n",
    "  ...  ...     (6 more candidates)\n",
    "\n",
    "\u2190 ALL 10 are handed to the cross-encoder for Stage 2 \u2192\n",
    "```\n",
    "\n",
    "**Why use a large first-stage k?**  \n",
    "The nearest-neighbor score (cosine similarity) measures vector direction, not\n",
    "deep semantic match.  A truly relevant chunk might be ranked 7th by cosine\n",
    "similarity but 1st after the cross-encoder reads the query and chunk together.\n",
    "Casting a wider net (k=10 instead of k=3) gives the reranker a better pool to\n",
    "work with at a small cost in latency.\n",
    "\n",
    "```\n",
    "Stage 2 \u2014 Reranker re-orders the 10 candidates:\n",
    "\n",
    " new rank  rerank score  original rank  chunk\n",
    " \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    1          0.94           4         tax implications \u2014 working abroad   \u2190 moved up!\n",
    "    2          0.91           1         remote work policy \u2014 full section\n",
    "    3          0.83           2         international transfer rules\n",
    "  (remaining 7 candidates dropped; only final_k=3 passed to LLM)\n",
    "```\n",
    "\n",
    "See **Tutorial 1 cells 10\u201313** for the step-by-step nearest-neighbor walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf296e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Retriever + reranker logic and novice score inspection\n",
    "\n",
    "def retrieve_with_rerank(question: str, first_stage_k: int = 10, final_k: int = 5):\n",
    "    first_pass = dense_retriever(question, top_k=first_stage_k)\n",
    "    reranked = reranker.rerank(question, first_pass, top_k=final_k)\n",
    "    return first_pass, reranked\n",
    "\n",
    "probe = \"What is the policy for working from another country?\"\n",
    "first_pass, reranked = retrieve_with_rerank(probe)\n",
    "\n",
    "before_df = pd.DataFrame([\n",
    "    {\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"dense_score\": r.score, \"preview\": r.text[:90]}\n",
    "    for i, r in enumerate(first_pass)\n",
    "])\n",
    "after_df = pd.DataFrame([\n",
    "    {\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"rerank_score\": r.score, \"preview\": r.text[:90]}\n",
    "    for i, r in enumerate(reranked)\n",
    "])\n",
    "\n",
    "print(\"Before reranking\")\n",
    "display(before_df.head(10))\n",
    "print(\"After reranking\")\n",
    "display(after_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8) Prompt + end-to-end RAG query\n",
    "\n",
    "def rag_answer_reranked(question: str, top_k: int = 5):\n",
    "    _, ranked = retrieve_with_rerank(question, first_stage_k=10, final_k=top_k)\n",
    "    context = [r.text for r in ranked]\n",
    "    answer = answer_with_context(question, context, model=chat_model)\n",
    "    return answer, ranked\n",
    "\n",
    "answer, ranked = rag_answer_reranked(probe)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470dabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10) Evaluation queries and debug output\n",
    "\n",
    "def retrieval_fn(question: str):\n",
    "    _, ranked = retrieve_with_rerank(question, first_stage_k=10, final_k=5)\n",
    "    return ranked\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=retrieval_fn,\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 3 metrics:\", summarize(rows))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}