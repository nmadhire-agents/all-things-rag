{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 9 - LLM Tracing with OpenTelemetry and Arize Phoenix\n\n## Where You Are in the Learning Journey\n\n```\n Tutorials 1-5      Tutorials 6-8      Tutorial 9\n RAG Fundamentals   Agent Extension    LLM Tracing\n (the retrieval     (ReAct, Reflection (you are here)\n  pipeline)          State Mgmt)\n```\n\n**What this tutorial adds:** observability for the RAG pipeline built in\nTutorials 1-8.  You will learn how to record every retrieval call, every LLM\ncall, and every end-to-end request as structured, searchable trace data.\n\n**What you will learn in this tutorial:**\n- What observability is and why LLM systems need it\n- The three OpenTelemetry building blocks: Spans, Traces, Exporters\n- How to create spans manually to record any operation\n- How to attach metadata (attributes) and error information to spans\n- How to nest spans to model parent-child relationships (e.g., one RAG request\n  contains one retrieval span and one generation span)\n- How to send traces to Arize Phoenix for visualization\n- How to auto-instrument OpenAI calls with one line using OpenInference\n\n**Prerequisites:** Tutorial 1 (dense RAG baseline).  Tutorials 2-8 are\nhelpful context but not required.\n\n```mermaid\nflowchart TD\n    Q[User Question] --> P[RAG Pipeline span]\n    P --> R[retrieval span]\n    P --> G[generation span]\n    R --> E1[Exporter]\n    G --> E1\n    P --> E1\n    E1 --> PH[Arize Phoenix UI]\n```\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What Is Observability and Why Do LLM Systems Need It?\n\n### The Problem with Black-Box LLM Systems\n\nWhen a RAG pipeline gives a wrong answer, you need to know **where the\nfailure happened**:\n\n- Did the retriever return irrelevant chunks?\n- Did the LLM ignore the context?\n- Did the embedding model fail on an unusual query?\n- Was a step unexpectedly slow?\n\nWithout instrumentation, the only thing you see is the final answer.\nWith instrumentation, you see a complete record of every step.\n\n### What Is Observability?\n\n**Observability** is the practice of making the internal state of a system\nvisible from the outside.  The three traditional pillars are:\n\n| Pillar | What it captures | Example |\n|--------|------------------|---------|\n| Logs   | Discrete events with a timestamp | `ERROR: retriever returned 0 results` |\n| Metrics | Numeric measurements over time | `retrieval_latency_ms = 230` |\n| Traces | Causally connected spans for one request | `RAG request -> retrieval -> generation` |\n\nThis tutorial focuses on **traces** because they are the most useful tool\nfor understanding *why* an individual request went wrong.\n\n### What Is OpenTelemetry?\n\n**OpenTelemetry (OTel)** is an open-source standard for producing and\ncollecting observability data.  It is vendor-neutral: you write your\ninstrumentation once and send the data to any backend (Arize Phoenix,\nJaeger, Honeycomb, Datadog, etc.).\n\nThe Python SDK (`opentelemetry-api` + `opentelemetry-sdk`) provides:\n- A `Tracer` object that creates spans\n- A `TracerProvider` that configures how spans are processed and exported\n- Exporters that forward finished spans to a backend over the network\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Core OpenTelemetry Concepts: Spans, Traces, and Exporters\n\n### Concept 1 - Span\n\nA **span** is the basic unit of a trace.  It represents **one named,\ntimed operation**.\n\nEvery span has:\n- A **name** that identifies the operation (e.g., `\"retrieval\"`)\n- A **start time** and **end time** (latency = end - start)\n- **Attributes** - key-value pairs attached as metadata\n- A **status** - either OK or ERROR\n- Optional **events** - timestamped notes added during execution\n\n```\nSpan: retrieval\n  start_time   : 2024-01-15 10:00:00.000\n  end_time     : 2024-01-15 10:00:00.230\n  duration     : 230 ms\n  attributes:\n    input.value          -> \"remote work VPN policy\"\n    retrieval.documents  -> 5\n  status       : OK\n```\n\n### Concept 2 - Trace\n\nA **trace** is a collection of spans connected by a shared **trace ID**.\nSpans within a trace can be nested: a **parent span** represents a\nhigh-level operation, and **child spans** represent sub-operations inside it.\n\n```\nTrace ID: abc123\n  rag-pipeline (parent)          0 ms - 450 ms\n    retrieval (child)            5 ms - 235 ms\n    generation (child)         235 ms - 450 ms\n```\n\nThe nesting is automatic: when you create a child span while a parent span\nis active in the current thread, OTel links them via the parent's span ID.\n\n### Concept 3 - Exporter\n\nAn **exporter** receives finished spans and forwards them to a backend.\nTwo exporters are used in this tutorial:\n\n| Exporter | When to use | Where spans go |\n|----------|-------------|----------------|\n| `ConsoleSpanExporter` | Development, learning | Printed to stdout |\n| `OTLPSpanExporter` | Production, visualization | Arize Phoenix (or any OTLP backend) |\n\n### Concept 4 - TracerProvider\n\nThe **TracerProvider** is the entry point for all tracing.  It:\n- Creates `Tracer` objects (one per component/library)\n- Holds the list of `SpanProcessor` objects (which call the exporter)\n- Attaches `Resource` metadata (e.g., service name) to every span\n\nYou configure the provider once at startup with `configure_tracing()`\nfrom the `rag_tutorials.tracing` module, then create tracers as needed.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib\nimport os\nfrom pathlib import Path\nimport shutil\nimport subprocess\nimport sys\n\nfrom dotenv import load_dotenv\n\nif shutil.which(\"uv\") is None:\n    print(\"uv not found. Installing with pip...\")\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n\ncwd = Path.cwd().resolve()\nrepo_root = next(\n    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n    cwd,\n)\nos.chdir(repo_root)\nsrc_path = repo_root / \"src\"\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\n\nREQUIRED_PACKAGES = [\n    \"openai\", \"chromadb\", \"numpy\", \"pandas\", \"rank_bm25\",\n    \"sentence_transformers\", \"dotenv\",\n    \"opentelemetry\", \"opentelemetry.sdk\",\n    \"openinference.instrumentation.openai\",\n]\nPIP_NAME_MAP = {\n    \"rank_bm25\": \"rank-bm25\",\n    \"sentence_transformers\": \"sentence-transformers\",\n    \"dotenv\": \"python-dotenv\",\n    \"opentelemetry\": \"opentelemetry-api\",\n    \"opentelemetry.sdk\": \"opentelemetry-sdk\",\n    \"openinference.instrumentation.openai\": \"openinference-instrumentation-openai\",\n}\n\ndef find_missing(packages):\n    importlib.invalidate_caches()\n    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n\nmissing = find_missing(REQUIRED_PACKAGES)\nif missing:\n    print(\"Missing packages:\", missing)\n    subprocess.run([\"uv\", \"sync\", \"--extra\", \"tracing\"], check=True)\n\nmissing_after_sync = find_missing(REQUIRED_PACKAGES)\nif missing_after_sync:\n    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n\nfinal_missing = find_missing([\"openai\", \"chromadb\", \"numpy\", \"pandas\"])\nif final_missing:\n    raise ImportError(f\"Dependencies still missing: {final_missing}\")\n\nfrom rag_tutorials.io_utils import load_handbook_documents, load_queries\nfrom rag_tutorials.chunking import semantic_chunk_documents\nfrom rag_tutorials.pipeline import build_dense_retriever\nfrom rag_tutorials.qa import answer_with_context\nfrom rag_tutorials.tracing import (\n    configure_tracing,\n    get_tracer,\n    traced_retrieval,\n    traced_generation,\n    build_traced_rag_pipeline,\n    ATTR_INPUT_VALUE,\n    ATTR_OUTPUT_VALUE,\n    ATTR_LLM_MODEL_NAME,\n    ATTR_RETRIEVAL_DOCUMENTS,\n)\n\nload_dotenv()\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n\nembedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\nchat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n\nhandbook_path = Path(\"data/handbook_manual.txt\")\nqueries_path = Path(\"data/queries.jsonl\")\nif not handbook_path.exists() or not queries_path.exists():\n    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n\ndocuments = load_handbook_documents(handbook_path)\nqueries = load_queries(queries_path)\nchunks = semantic_chunk_documents(documents)\ndense_retriever, _ = build_dense_retriever(\n    chunks=chunks,\n    collection_name=\"tracing_tutorial_dense\",\n    embedding_model=embedding_model,\n)\n\nprint(\"Setup complete.\")\nprint(f\"  Documents  : {len(documents)}\")\nprint(f\"  Chunks     : {len(chunks)}\")\nprint(f\"  Queries    : {len(queries)}\")\nprint(f\"  Chat model : {chat_model}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1 - Manual Span Creation (No Backend Required)\n\nBefore connecting to Phoenix, learn how spans work by creating them\nmanually and reading the output printed to stdout.\n\nThe three steps for any span:\n\n1. Call `configure_tracing()` to set up the provider (once per session)\n2. Call `get_tracer(name)` to get a tracer for your component\n3. Use `tracer.start_as_current_span(name)` as a context manager around\n   the code you want to measure\n\nAnything inside the `with` block is timed automatically.  Attributes are\nkey-value pairs you attach to describe what the span measured.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1 - Set up the provider.\n# With no arguments, spans are printed to stdout (ConsoleSpanExporter).\n# This is useful for learning and development.\n\nconfigure_tracing(service_name=\"rag-tracing-tutorial\")\n\n# Step 2 - Get a tracer.  The name is a label for this component.\ntracer = get_tracer(\"tutorial.manual\")\n\n# Step 3 - Create a span.\n# 'with tracer.start_as_current_span(name) as span:' starts the timer.\n# The span ends automatically when the 'with' block exits.\n\nwith tracer.start_as_current_span(\"first-span\") as span:\n    # Attach metadata with set_attribute(key, value).\n    # Keys are plain strings; values can be strings, ints, or floats.\n    span.set_attribute(\"example.greeting\", \"hello from the span\")\n    span.set_attribute(\"example.number\", 42)\n\n    # Your normal code goes here.\n    result = 6 * 7\n    span.set_attribute(\"example.result\", result)\n\n# After the 'with' block, the span is finished and exported.\n# Look for 'first-span' in the JSON output above.\nprint(f\"Span finished. Computed result: {result}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Reading the Console Exporter Output\n\nThe JSON block printed above contains the complete span.  Key fields to\nlook for:\n\n| JSON field | What it means |\n|------------|---------------|\n| `name` | The span name you passed to `start_as_current_span` |\n| `start_time` | When the span started (nanosecond timestamp) |\n| `end_time` | When the span ended |\n| `attributes` | Key-value pairs attached with `set_attribute` |\n| `status` | `StatusCode.OK` when no error was recorded |\n| `context.trace_id` | Unique ID shared by all spans in one trace |\n| `context.span_id` | Unique ID for this individual span |\n| `parent_id` | Span ID of the parent (None for a root span) |\n| `resource` | Service-level metadata (service name, etc.) |\n\nThe `trace_id` links spans that belong to the same request.  When you\ncreate a child span inside a parent span, the child automatically gets the\nsame `trace_id` and records the parent's `span_id` as its `parent_id`.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Parent-Child Span Relationships\n\nNesting is the key feature that makes traces useful.  A parent span\nrepresents the whole operation; child spans represent sub-steps inside it.\n\nThe rule is simple: **any span started while another span is active becomes\nits child automatically.**  There is no explicit linking call required.\n\nThe cell below creates a `rag-request` parent with `retrieval` and\n`generation` children, mirroring what the RAG pipeline actually does.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parent-child span demo\n# Create a root span for the whole request, then child spans for each step.\n\nwith tracer.start_as_current_span(\"rag-request\") as root_span:\n    root_span.set_attribute(ATTR_INPUT_VALUE, \"What is the VPN policy?\")\n\n    # The retrieval child span is nested inside rag-request.\n    with tracer.start_as_current_span(\"retrieval\") as retrieval_span:\n        retrieval_span.set_attribute(ATTR_INPUT_VALUE, \"VPN policy\")\n        # Simulating work\n        retrieval_span.set_attribute(ATTR_RETRIEVAL_DOCUMENTS, 5)\n\n    # The generation child span is also nested inside rag-request.\n    with tracer.start_as_current_span(\"generation\") as generation_span:\n        generation_span.set_attribute(ATTR_LLM_MODEL_NAME, chat_model)\n        generation_span.set_attribute(ATTR_INPUT_VALUE, \"What is the VPN policy?\")\n        # Simulating work\n        generation_span.set_attribute(ATTR_OUTPUT_VALUE, \"VPN is required for all remote access.\")\n\nprint(\"Trace complete.\")\nprint(\"Look for rag-request, retrieval, and generation spans above.\")\nprint(\"Notice that retrieval and generation share the same trace_id as rag-request.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Span Events and Error Recording\n\nA **span event** is a timestamped note added inside an active span.  Use\nevents to mark significant moments that are not operations in their own\nright (for example, \"cache miss\" or \"context truncated\").\n\nWhen an exception occurs inside a span, you can record it with\n`span.record_exception(exc)` and set the status to ERROR.  This lets the\nobservability backend filter to failed requests.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from opentelemetry.trace import StatusCode\n\n# Demo 1: span events\nwith tracer.start_as_current_span(\"retrieval-with-events\") as span:\n    span.set_attribute(ATTR_INPUT_VALUE, \"international work limit\")\n\n    # add_event records a note at the current timestamp\n    span.add_event(\"cache_miss\", {\"cache_key\": \"international work limit\"})\n\n    # Simulate fetching from the retriever\n    span.add_event(\"retrieved\", {\"count\": 3})\n    span.set_attribute(ATTR_RETRIEVAL_DOCUMENTS, 3)\n\nprint(\"Span with events written.  Look for 'events' in the output.\")\n\n# Demo 2: error recording\nimport contextlib\n\nwith tracer.start_as_current_span(\"retrieval-with-error\") as span:\n    span.set_attribute(ATTR_INPUT_VALUE, \"leave policy\")\n    with contextlib.suppress(ValueError):\n        try:\n            raise ValueError(\"Chroma connection failed\")\n        except ValueError as exc:\n            span.record_exception(exc)\n            span.set_status(StatusCode.ERROR, str(exc))\n            raise\n\nprint(\"Error span written.  Look for 'status': 'StatusCode.ERROR' in the output.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2 - Arize Phoenix Setup\n\n### What Is Arize Phoenix?\n\n**Arize Phoenix** is an open-source LLM observability platform.  It\nprovides:\n- A local web server that collects traces sent via the OTLP protocol\n- A UI to search, filter, and inspect every span\n- Built-in support for LLM-specific attributes (prompt, response, model)\n- The ability to replay and compare requests side by side\n\nPhoenix speaks the same OTLP protocol as the OTel SDK, so you can switch\nfrom the console exporter to Phoenix by changing one argument in\n`configure_tracing`.\n\n### Starting Phoenix Locally\n\nRun this command in a terminal (not in the notebook) to start Phoenix:\n\n```bash\nuv run python -m phoenix.server.main\n```\n\nPhoenix starts on port 6006 by default.  Open `http://localhost:6006` in\nyour browser to see the UI.  Leave the terminal running while you use the\nnotebook.\n\nYou can also start Phoenix programmatically from inside the notebook using\nthe `arize-phoenix-otel` package.  The cell below shows both approaches.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# To send traces to Phoenix, change the configure_tracing call to include\n# the Phoenix OTLP endpoint:\n#\n#   configure_tracing(\n#       endpoint=\"http://localhost:6006/v1/traces\",\n#       service_name=\"rag-tracing-tutorial\",\n#   )\n#\n# The rest of the notebook code is identical regardless of which backend\n# you use.  The exporter is the only thing that changes.\n#\n# Alternatively, use arize-phoenix-otel for a one-line setup that also\n# starts a local Phoenix server and registers the exporter:\n#\n#   import phoenix as px\n#   from phoenix.otel import register\n#\n#   px.launch_app()   # starts Phoenix at http://localhost:6006\n#   register()        # configures the global OTel provider for Phoenix\n#\n# For this notebook the ConsoleSpanExporter (already configured above)\n# is used so you can run everything without a running Phoenix instance.\n\nprint(\"Phoenix setup notes printed.\")\nprint(\"To enable Phoenix: uncomment the configure_tracing call with the endpoint.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3 - Tracing the RAG Pipeline with the Shared Module\n\nThe `rag_tutorials.tracing` module provides three ready-to-use wrappers\nthat you can drop in around any existing retriever or answer function:\n\n| Function | What it wraps | Span name created |\n|----------|--------------|-------------------|\n| `traced_retrieval(retriever, tracer)` | Any retriever callable | `retrieval` |\n| `traced_generation(answer_fn, tracer)` | Any generation callable | `generation` |\n| `build_traced_rag_pipeline(retriever, answer_fn, tracer)` | Full pipeline | `rag-pipeline` (parent) + two children |\n\nEach wrapper function takes your existing callable and returns a new callable\nwith identical input/output behaviour.  The only difference is that every\ncall now creates an OTel span.\n\n### Novice Trace: Watching a Real Retrieval Span\n\nThe cell below wraps the dense retriever from Tutorial 1 with\n`traced_retrieval` and runs one query.  Watch the console output to see\nthe span that is created.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Wrap the dense retriever with a tracing span.\n# The wrapper has the same interface as dense_retriever:\n#   (question: str, top_k: int = ...) -> list[RetrievalResult]\n\nretrieval_tracer = get_tracer(\"tutorial.retrieval\")\ntraced_dense = traced_retrieval(dense_retriever, retrieval_tracer)\n\nquestion = \"What is the maximum number of days an employee can work internationally?\"\n\nprint(\"Running retrieval with tracing...\")\nprint(f\"Question: {question}\")\nprint()\n\nresults = traced_dense(question, top_k=3)\n\nprint(f\"Retrieved {len(results)} chunks:\")\nfor i, r in enumerate(results, start=1):\n    print(f\"  Chunk {i} [{r.chunk_id}] (score={r.score:.3f}): {r.text[:80]}...\")\n\nprint()\nprint(\"Look in the console output above this cell for the 'retrieval' span JSON.\")\nprint(f\"The span should show input.value='{question[:50]}...' and retrieval.documents=3.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Running the Full Traced Pipeline\n\nNow build the end-to-end traced pipeline using `build_traced_rag_pipeline`.\nThis creates three spans for every question:\n1. A parent `rag-pipeline` span that contains the whole request\n2. A child `retrieval` span (inside the parent)\n3. A child `generation` span (inside the parent)\n\nThe parent and child spans all share the same `trace_id`, so Phoenix (and\nany other OTLP backend) can display them as a single flame-graph view.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a fully traced RAG pipeline from the shared modules.\n\npipeline_tracer = get_tracer(\"tutorial.pipeline\")\n\npipeline = build_traced_rag_pipeline(\n    retriever=dense_retriever,\n    answer_fn=answer_with_context,\n    tracer=pipeline_tracer,\n    model_name=chat_model,\n)\n\nquestion = \"What must an employee do before working from another country for more than 14 days?\"\n\nprint(\"Running full traced pipeline...\")\nprint(f\"Question: {question}\")\nprint()\n\nanswer = pipeline(question)\n\nprint(\"ANSWER:\", answer)\nprint()\nprint(\"The console output above contains three spans:\")\nprint(\"  1. 'retrieval'   - child span for the retrieval step\")\nprint(\"  2. 'generation'  - child span for the LLM call\")\nprint(\"  3. 'rag-pipeline'- parent span wrapping both children\")\nprint(\"All three spans share the same trace_id.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4 - Auto-Instrumentation with OpenInference\n\n### What Is OpenInference?\n\n**OpenInference** is a set of OTel instrumentation libraries for popular\nLLM frameworks.  Instead of manually adding spans everywhere, you call one\nsetup function and every LLM call in your process is traced automatically.\n\nThe `openinference-instrumentation-openai` package instruments the OpenAI\nPython SDK.  After calling `OpenAIInstrumentor().instrument()`, every\n`client.chat.completions.create(...)` call produces a span that records:\n\n| Attribute | What it captures |\n|-----------|------------------|\n| `llm.model_name` | The model used (e.g. `gpt-4.1-mini`) |\n| `input.value` | The full prompt sent to the model |\n| `output.value` | The full response from the model |\n| `llm.token_count.prompt` | Number of prompt tokens |\n| `llm.token_count.completion` | Number of completion tokens |\n\nThis means you get detailed LLM call tracing without modifying any of the\ncode that calls the OpenAI client.\n\n### Important: Call Order\n\nYou must call `configure_tracing()` (or Phoenix's `register()`) **before**\ncalling `OpenAIInstrumentor().instrument()`.  The instrumentor patches the\nOpenAI SDK so that every new call reports to the currently active\nTracerProvider.  If the provider is not set yet, spans are silently dropped.\n\n```\nCorrect order:\n  1. configure_tracing(endpoint=...)   or   register()\n  2. OpenAIInstrumentor().instrument()\n  3. Your application code\n```\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enable automatic OpenAI instrumentation.\n# After this call, every client.chat.completions.create() produces a span\n# with the prompt, response, model name, and token counts included.\n\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\n# configure_tracing was already called in the setup cell above,\n# so the provider is already active.  Just instrument OpenAI now.\nOpenAIInstrumentor().instrument()\n\nprint(\"OpenAI auto-instrumentation is active.\")\nprint(\"Every OpenAI API call from this point on will produce a span.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run one question through the pipeline.\n# Because OpenAIInstrumentor is active, the internal OpenAI call inside\n# answer_with_context will produce an additional automatic span with the\n# full prompt and response captured.\n\nauto_tracer = get_tracer(\"tutorial.auto\")\n\nauto_pipeline = build_traced_rag_pipeline(\n    retriever=dense_retriever,\n    answer_fn=answer_with_context,\n    tracer=auto_tracer,\n    model_name=chat_model,\n)\n\nquestion = \"What is the remote work VPN requirement?\"\n\nprint(f\"Question: {question}\")\nprint()\n\nanswer = auto_pipeline(question)\n\nprint(\"ANSWER:\", answer)\nprint()\nprint(\"Console output now shows an additional span from OpenAI auto-instrumentation.\")\nprint(\"That span captures the full prompt text and token usage automatically.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparing Console Output to a Phoenix UI View\n\nWhen you switch `configure_tracing` to use the Phoenix endpoint:\n\n```python\nconfigure_tracing(\n    endpoint=\"http://localhost:6006/v1/traces\",\n    service_name=\"rag-tracing-tutorial\",\n)\n```\n\nEvery span in the console output above would instead appear in the Phoenix\nUI at `http://localhost:6006`.  The UI shows:\n\n- A **Traces** table listing every request with its latency and status\n- A **Trace Detail** view with a flame graph showing the parent and child\n  spans side by side\n- **Attribute inspection**: click any span to see all its attributes\n- **Token usage charts**: total prompt and completion tokens over time\n- **Error filtering**: filter to only show traces with ERROR status\n\nThe data you see in Phoenix is exactly the same as what the console exporter\nprints - only the presentation is different.\n\n### Switching Between Backends\n\nThe pattern for switching from console to Phoenix is:\n\n```\nDevelopment  -> configure_tracing()                         (stdout)\nLocal UI     -> configure_tracing(endpoint=\"http://localhost:6006/v1/traces\")\nCloud        -> configure_tracing(endpoint=\"https://<your-phoenix-cloud>/v1/traces\")\n```\n\nEverything else (tracer creation, span wrappers, instrumentors) stays the same.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Learning Checkpoint: LLM Tracing\n\n### What Works\n\n- The `configure_tracing` / `get_tracer` / `start_as_current_span` pattern\n  gives you precise timing and metadata for any operation with very little\n  code added.\n- Parent-child span nesting is automatic: creating a span inside an active\n  span links them by trace ID without any explicit join call.\n- The `traced_retrieval` and `traced_generation` wrappers in\n  `rag_tutorials.tracing` instrument the full RAG pipeline with two function\n  calls.\n- `OpenAIInstrumentor().instrument()` captures the complete prompt and\n  response for every OpenAI API call without touching the call sites.\n- Switching from the console exporter to Phoenix requires changing one\n  argument in `configure_tracing`; all other code stays identical.\n\n### What Does Not Work Out of the Box\n\n- The console exporter is good for learning but hard to query at scale.\n  Phoenix (or another OTLP backend) is needed for production use.\n- The wrappers in this tutorial record span-level metadata but not\n  chunk-level embedding vectors or full retrieved text.  Storing those\n  requires additional `set_attribute` calls inside `traced_retrieval`.\n- OTel context propagation across async tasks or multiple processes\n  requires additional setup (context propagators, HTTP header injection).\n\n### What You Have Learned\n\n- Span: the unit of a trace; one named, timed, attributed operation\n- Trace: a tree of spans sharing a trace ID; represents one request\n- TracerProvider: configures how spans are created and exported\n- Exporter: receives finished spans and sends them to a backend\n- Auto-instrumentation: patches a library at runtime so every call is traced\n- Arize Phoenix: open-source OTLP backend with a UI for LLM traces\n"
  }
 ]
}