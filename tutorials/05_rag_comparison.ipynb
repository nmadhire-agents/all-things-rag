{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0809845d",
   "metadata": {},
   "source": [
    "# Tutorial 5 â€” Side-by-Side RAG Benchmark\n",
    "\n",
    "This notebook compares the four tutorial variants under the same conditions:\n",
    "\n",
    "1. Dense baseline + fixed chunks\n",
    "2. Dense + semantic chunks\n",
    "3. Dense + semantic + reranking\n",
    "4. Hybrid (dense + BM25) + semantic chunks\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Shared Documents + Queries] --> B1[Variant 1]\n",
    "    A --> B2[Variant 2]\n",
    "    A --> B3[Variant 3]\n",
    "    A --> B4[Variant 4]\n",
    "    B1 --> C[Common Evaluation Harness]\n",
    "    B2 --> C\n",
    "    B3 --> C\n",
    "    B4 --> C\n",
    "    C --> D[Benchmark Table + Plots]\n",
    "```\n",
    "\n",
    "Outcome: one consolidated table with `Recall@k`, `MRR`, `Groundedness`, and `Latency`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd651d6",
   "metadata": {},
   "source": [
    "## Learning checkpoint: how to read benchmark outcomes\n",
    "\n",
    "**What this notebook confirms**\n",
    "- Which variant is best for your workload depends on metric priority.\n",
    "- Retrieval quality, groundedness, and latency can move in different directions.\n",
    "\n",
    "**Common interpretation mistakes to avoid**\n",
    "- Picking a winner from one metric only (e.g., MRR without latency).\n",
    "- Ignoring failure cases where a variant has blind spots.\n",
    "- Assuming the best mean score is best for every query type.\n",
    "\n",
    "**How to use results for next iteration**\n",
    "- Select a baseline variant from measured tradeoffs.\n",
    "- Tune top-k, chunking, reranking depth, and hybrid fusion.\n",
    "- Add domain-specific queries to stress-test your production use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0274f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup and imports\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure uv is available (installs with: pip install uv)\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "# Ensure notebook runs from repo root and local src/ is importable\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "    \"seaborn\",\n",
    "    \"matplotlib\",\n",
    "]\n",
    "\n",
    "missing = [pkg for pkg in REQUIRED_PACKAGES if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "else:\n",
    "    print(\"All required packages are available.\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import fixed_chunk_documents, semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever, build_hybrid_retriever\n",
    "from rag_tutorials.reranking import LocalCrossEncoderReranker\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is missing. Configure .env before running this notebook.\")\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "TOP_K = 5\n",
    "EVAL_SIZE = 30\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Setup complete\")\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"Using src path:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load shared handbook text + query set from data/\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Shared data files are missing. Run: uv run python scripts/generate_data.py\"\n",
    "    )\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)[:EVAL_SIZE]\n",
    "\n",
    "print({\"handbook_path\": str(handbook_path), \"parsed_sections\": len(documents), \"eval_queries\": len(queries)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Build all four retrieval variants\n",
    "\n",
    "fixed_chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "semantic_chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "v1_dense_fixed, _ = build_dense_retriever(\n",
    "    chunks=fixed_chunks,\n",
    "    collection_name=\"t5_v1_dense_fixed\",\n",
    "    embedding_model=EMBED_MODEL,\n",
    ")\n",
    "\n",
    "v2_dense_semantic, _ = build_dense_retriever(\n",
    "    chunks=semantic_chunks,\n",
    "    collection_name=\"t5_v2_dense_semantic\",\n",
    "    embedding_model=EMBED_MODEL,\n",
    ")\n",
    "\n",
    "reranker = LocalCrossEncoderReranker()\n",
    "\n",
    "def v3_reranked(question: str, top_k: int = TOP_K):\n",
    "    first_pass = v2_dense_semantic(question, top_k=10)\n",
    "    return reranker.rerank(question, first_pass, top_k=top_k)\n",
    "\n",
    "v4_hybrid_semantic = build_hybrid_retriever(semantic_chunks, v2_dense_semantic)\n",
    "\n",
    "variants = {\n",
    "    \"t1_dense_fixed\": v1_dense_fixed,\n",
    "    \"t2_dense_semantic\": v2_dense_semantic,\n",
    "    \"t3_reranked\": v3_reranked,\n",
    "    \"t4_hybrid\": v4_hybrid_semantic,\n",
    "}\n",
    "\n",
    "print(\"Variants ready:\", list(variants.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deacab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Evaluate every variant with identical queries\n",
    "\n",
    "def answer_fn(question: str, contexts: list[str]) -> str:\n",
    "    return answer_with_context(question, contexts, model=CHAT_MODEL)\n",
    "\n",
    "summary_rows = []\n",
    "all_rows = []\n",
    "\n",
    "for name, retrieval_fn in variants.items():\n",
    "    started = time.perf_counter()\n",
    "    rows = [\n",
    "        evaluate_single(\n",
    "            query=q,\n",
    "            retrieval_fn=lambda query_text, fn=retrieval_fn: fn(query_text, top_k=TOP_K),\n",
    "            answer_fn=answer_fn,\n",
    "            top_k=TOP_K,\n",
    "        )\n",
    "        for q in queries\n",
    "    ]\n",
    "    elapsed = time.perf_counter() - started\n",
    "    metrics = summarize(rows)\n",
    "    metrics[\"variant\"] = name\n",
    "    metrics[\"wall_seconds\"] = elapsed\n",
    "    summary_rows.append(metrics)\n",
    "\n",
    "    for row in rows:\n",
    "        all_rows.append(\n",
    "            {\n",
    "                \"variant\": name,\n",
    "                \"query_id\": row.query_id,\n",
    "                \"recall_at_k\": row.recall_at_k,\n",
    "                \"mrr\": row.mrr,\n",
    "                \"latency_ms\": row.latency_ms,\n",
    "                \"groundedness\": row.groundedness,\n",
    "            }\n",
    "        )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"mrr\", ascending=False)\n",
    "detail_df = pd.DataFrame(all_rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Plot benchmark comparison\n",
    "\n",
    "melted = summary_df.melt(\n",
    "    id_vars=[\"variant\"],\n",
    "    value_vars=[\"recall_at_k\", \"mrr\", \"groundedness\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"value\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=melted, x=\"metric\", y=\"value\", hue=\"variant\")\n",
    "plt.title(\"RAG Variant Quality Comparison\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=summary_df, x=\"variant\", y=\"latency_ms\")\n",
    "plt.title(\"Average End-to-End Latency (ms)\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce03815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Failure analysis helper: show where one variant succeeds and another fails\n",
    "\n",
    "pivot_recall = detail_df.pivot_table(index=\"query_id\", columns=\"variant\", values=\"recall_at_k\")\n",
    "\n",
    "if {\"t1_dense_fixed\", \"t4_hybrid\"}.issubset(set(pivot_recall.columns)):\n",
    "    hard_cases = pivot_recall[(pivot_recall[\"t1_dense_fixed\"] < 1.0) & (pivot_recall[\"t4_hybrid\"] == 1.0)]\n",
    "    print(\"Queries where hybrid recovers misses from baseline:\", len(hard_cases))\n",
    "    display(hard_cases.head(10))\n",
    "else:\n",
    "    print(\"Expected variants not found in detail table.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
