{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b5c254",
   "metadata": {},
   "source": [
    "# Tutorial 4 \u2014 Hybrid Search (Dense + Keyword)\n",
    "\n",
    "This final tutorial adds BM25 keyword retrieval and fuses it with dense retrieval via Reciprocal Rank Fusion.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Q[Query] --> D[Dense Retriever]\n",
    "    Q --> K[Keyword Retriever BM25]\n",
    "    D --> F[RRF Fusion]\n",
    "    K --> F\n",
    "    F --> L[Top-k for Generation]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950df206",
   "metadata": {},
   "source": [
    "## Learning checkpoint: hybrid tradeoffs and readiness for benchmarking\n",
    "\n",
    "**What works better in Tutorial 4**\n",
    "- Keyword retrieval improves exact-token recall (e.g., `Form A-12`).\n",
    "- Dense retrieval still contributes semantic context.\n",
    "- Fusion provides more robust retrieval across question styles.\n",
    "\n",
    "**Challenges you should observe**\n",
    "- More moving parts increase tuning complexity (weights, top-k, fusion behavior).\n",
    "- Latency and system complexity are higher than baseline.\n",
    "- Hybrid is better in many cases, but not always dominant on every metric.\n",
    "\n",
    "**Why move to Tutorial 5**\n",
    "- At this point, intuition is not enough.\n",
    "- We benchmark all variants side-by-side to quantify quality, groundedness, and latency tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-5) Setup, handbook text, semantic chunks, embeddings, index\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\"openai\", \"chromadb\", \"numpy\", \"pandas\", \"rank_bm25\", \"sentence_transformers\", \"dotenv\"]\n",
    "PIP_NAME_MAP = {\"rank_bm25\": \"rank-bm25\", \"sentence_transformers\": \"sentence-transformers\", \"dotenv\": \"python-dotenv\"}\n",
    "\n",
    "def find_missing(packages: list[str]) -> list[str]:\n",
    "    importlib.invalidate_caches()\n",
    "    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    print(\"Installing into current kernel with pip:\", pip_targets)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing in current kernel: {final_missing}\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever, build_hybrid_retriever\n",
    "from rag_tutorials.retrieval import build_bm25, bm25_search\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)\n",
    "chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "dense_retriever, _ = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial4_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "hybrid_retriever = build_hybrid_retriever(chunks, dense_retriever)\n",
    "\n",
    "bm25_index, corpus, chunk_ids = build_bm25(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4_vector_ref",
   "metadata": {},
   "source": [
    "### Two Retrieval Signals: Cosine Similarity (dense) and BM25 (keyword)\n",
    "\n",
    "**Dense retrieval \u2014 cosine similarity (Tutorial 1 baseline)**  \n",
    "Query and chunk texts are converted to embedding vectors; chunks are ranked by cosine similarity.\n",
    "Captures *semantic* similarity \u2014 works well even when the exact words differ.\n",
    "\n",
    "**BM25 keyword retrieval (new in this tutorial)**  \n",
    "Ranks chunks by term-frequency statistics.  \n",
    "Score \u221d how often query terms appear in the chunk, normalized by chunk length.  \n",
    "Captures *lexical* similarity \u2014 reliable for exact identifiers like `Form A-12`.\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF)**  \n",
    "Both ranked lists are merged: `score = 1/(k + rank_dense) + 1/(k + rank_bm25)`.  \n",
    "Chunks that rank well in *both* lists receive the highest combined score.\n",
    "\n",
    "See **Tutorial 1, cells 8\u201310** for the cosine similarity step-by-step walkthrough.",
    "\n",
    "#### Side-by-side toy example: how each signal produces its own top-k\n",
    "\n",
    "Both dense and keyword retrieval independently rank all chunks and return a top-k list.\n",
    "Reciprocal Rank Fusion (RRF) then combines the two ranked lists into a single one.\n",
    "\n",
    "```\n",
    "Query: 'Form A-12 reimbursement'\n",
    "\n",
    "Dense retrieval (cosine similarity)   BM25 keyword retrieval\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "rank 1 [0.88]  expense policy         rank 1 [9.2]  Form A-12 instructions   \u2190 exact match!\n",
    "rank 2 [0.81]  reimbursement guide    rank 2 [7.8]  reimbursement form guide\n",
    "rank 3 [0.74]  travel budget rules    rank 3 [5.1]  expense policy\n",
    "rank 4 [0.68]  form submission guide  rank 4 [3.4]  form submission guide\n",
    "rank 5 [0.61]  HR contact info        rank 5 [1.9]  travel budget rules\n",
    "\n",
    "Dense misses 'Form A-12 instructions' at rank 1 \u2014 the exact form name doesn't\n",
    "appear in its training data, so the embedding is not as close as BM25's match.\n",
    "\n",
    "RRF fusion (k=60 constant):  score = 1/(60+rank_dense) + 1/(60+rank_bm25)\n",
    "\n",
    " chunk                        RRF score  final rank\n",
    " \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    " Form A-12 instructions       0.01613    1   \u2190 keyword signal promoted it\n",
    " expense policy               0.01590    2\n",
    " reimbursement form guide     0.01573    3\n",
    " form submission guide        0.01524    4\n",
    " travel budget rules          0.01506    5\n",
    "```\n",
    "\n",
    "BM25's strong exact-match signal for 'Form A-12' lifts that chunk to the top even\n",
    "though dense retrieval ranked it only 1st \u2014 both happen to agree here, but in\n",
    "cases where they disagree (one scores high, the other low) RRF balances them.\n",
    "\n",
    "See **Tutorial 1 cells 10\u201313** for the nearest-neighbor + cosine similarity walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Implement retrieval functions and inspect dense vs keyword vs hybrid\n",
    "\n",
    "def dense_only(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "def keyword_only(question: str, top_k: int = 5):\n",
    "    return bm25_search(bm25_index, question, corpus, chunk_ids, top_k=top_k)\n",
    "\n",
    "def hybrid(question: str, top_k: int = 5):\n",
    "    return hybrid_retriever(question, top_k=top_k)\n",
    "\n",
    "probe = \"Do I need Form A-12 for my trip?\"\n",
    "\n",
    "dense_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(dense_only(probe))])\n",
    "keyword_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(keyword_only(probe))])\n",
    "hybrid_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(hybrid(probe))])\n",
    "\n",
    "print(\"Dense only\")\n",
    "display(dense_df)\n",
    "print(\"Keyword only\")\n",
    "display(keyword_df)\n",
    "print(\"Hybrid\")\n",
    "display(hybrid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a79250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8) Prompt assembly + end-to-end query\n",
    "\n",
    "def rag_answer_hybrid(question: str, top_k: int = 5):\n",
    "    retrieved = hybrid(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "answer, retrieved = rag_answer_hybrid(probe)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b985ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10) Evaluation + continuity summary table\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: hybrid(question, top_k=5),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 4 metrics:\", summarize(rows))\n",
    "\n",
    "continuity = pd.DataFrame(\n",
    "    [\n",
    "        {\"tutorial\": 1, \"change\": \"dense baseline with fixed chunks\"},\n",
    "        {\"tutorial\": 2, \"change\": \"semantic chunking\"},\n",
    "        {\"tutorial\": 3, \"change\": \"reranking\"},\n",
    "        {\"tutorial\": 4, \"change\": \"hybrid dense + keyword\"},\n",
    "    ]\n",
    ")\n",
    "continuity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}