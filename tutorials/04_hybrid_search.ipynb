{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b5c254",
   "metadata": {},
   "source": [
    "# Tutorial 4 \u2014 Hybrid Search (Dense + Keyword)\n",
    "\n",
    "## Where You Are in the Learning Journey\n",
    "\n",
    "```\n",
    " Tutorial 1 --> Tutorial 2 --> Tutorial 3 --> Tutorial 4 --> Tutorial 5\n",
    " Basic RAG      Semantic       Reranking      YOU ARE        Benchmark\n",
    " (baseline)     Chunking       (T3)           HERE           All Four\n",
    "```\n",
    "\n",
    "**What changed from Tutorial 3:** a second retrieval signal \u2014 BM25 keyword search \u2014\n",
    "is added and fused with dense retrieval via Reciprocal Rank Fusion (RRF).\n",
    "\n",
    "**What you will learn in this tutorial:**\n",
    "- What BM25 is and how it scores documents using term frequency\n",
    "- Why keyword retrieval catches things that dense retrieval misses\n",
    "- What Reciprocal Rank Fusion (RRF) is and why it combines two ranked lists well\n",
    "- When hybrid search helps and when it does not\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Q[Query] --> D[Dense Retriever]\n",
    "    Q --> K[Keyword Retriever BM25]\n",
    "    D --> F[RRF Fusion]\n",
    "    K --> F\n",
    "    F --> L[Top-k for Generation]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950df206",
   "metadata": {},
   "source": [
    "## Learning Checkpoint: Hybrid Tradeoffs and Readiness for Benchmarking\n",
    "\n",
    "### What Works Better in Tutorial 4\n",
    "\n",
    "- **Keyword retrieval** improves exact-token recall (e.g. searching for 'Form A-12'\n",
    "  or a specific regulation code finds the exact chunk even when embeddings are weak).\n",
    "- **Dense retrieval** still contributes semantic context (synonyms, paraphrases).\n",
    "- **Fusion** provides more robust retrieval across different question styles.\n",
    "\n",
    "### What Still Does Not Work Well\n",
    "\n",
    "- More moving parts increase tuning complexity (BM25 weights, top-k per source,\n",
    "  fusion constant k).\n",
    "- Latency and system complexity are higher than Tutorial 1.\n",
    "- Hybrid is better in *most* cases but not all \u2014 Tutorial 5 shows the data.\n",
    "\n",
    "### Why Move to Tutorial 5?\n",
    "\n",
    "We now have four variants.  Tutorial 5 runs all four under identical conditions\n",
    "and compares them on the same metrics.  That is how we decide which architecture\n",
    "to deploy for a real system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-5) Setup, handbook text, semantic chunks, embeddings, index\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\"openai\", \"chromadb\", \"numpy\", \"pandas\", \"rank_bm25\", \"sentence_transformers\", \"dotenv\"]\n",
    "PIP_NAME_MAP = {\"rank_bm25\": \"rank-bm25\", \"sentence_transformers\": \"sentence-transformers\", \"dotenv\": \"python-dotenv\"}\n",
    "\n",
    "def find_missing(packages: list[str]) -> list[str]:\n",
    "    importlib.invalidate_caches()\n",
    "    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    print(\"Installing into current kernel with pip:\", pip_targets)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing in current kernel: {final_missing}\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever, build_hybrid_retriever\n",
    "from rag_tutorials.retrieval import build_bm25, bm25_search\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)\n",
    "chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "dense_retriever, _ = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial4_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "hybrid_retriever = build_hybrid_retriever(chunks, dense_retriever)\n",
    "\n",
    "bm25_index, corpus, chunk_ids = build_bm25(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08d49e",
   "metadata": {},
   "source": [
    "### What Is BM25 and How Does It Score Documents?\n",
    "\n",
    "**BM25** stands for *Best Match 25*.  It is a keyword-based ranking algorithm\n",
    "that has been the standard in search engines for decades.\n",
    "Unlike dense retrieval (which uses vector similarity), BM25 scores documents\n",
    "purely based on which words they share with the query.\n",
    "\n",
    "#### The Core Idea: Term Frequency and Document Frequency\n",
    "\n",
    "BM25 cares about two things:\n",
    "\n",
    "1. **Term Frequency (TF)** \u2014 How many times does the query word appear in this chunk?\n",
    "   More occurrences = higher score, but with diminishing returns (a word appearing\n",
    "   10 times is not 10x better than once).\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)** \u2014 How rare is this word across *all* chunks?\n",
    "   Rare words (e.g. 'repatriation') are more informative than common words (e.g. 'the').\n",
    "   A rare word appearing in a chunk is a stronger signal of relevance.\n",
    "\n",
    "```\n",
    "Query: \"Form A-12 reimbursement\"\n",
    "\n",
    "Chunk A: \"Submit Form A-12 to request reimbursement for relocation costs.\"\n",
    "  -> \"Form\" appears 1x   IDF: medium  (fairly common word)\n",
    "  -> \"A-12\" appears 1x   IDF: HIGH    (very rare \u2014 almost unique)\n",
    "  -> \"reimbursement\" appears 1x  IDF: medium\n",
    "  -> BM25 score: HIGH  (rare term 'A-12' matches exactly)\n",
    "\n",
    "Chunk B: \"Employees may request reimbursement for approved expenses.\"\n",
    "  -> \"Form\" appears 0x   (no match for this term)\n",
    "  -> \"A-12\" appears 0x   (no match)\n",
    "  -> \"reimbursement\" appears 1x\n",
    "  -> BM25 score: LOW  (only one of three query terms matched)\n",
    "```\n",
    "\n",
    "#### Dense vs BM25 \u2014 What Each Is Good At\n",
    "\n",
    "| Scenario | Dense (embedding) | BM25 (keyword) |\n",
    "|----------|-------------------|----------------|\n",
    "| Query uses different words than the document | Good (understands synonyms) | Poor (needs exact match) |\n",
    "| Query contains a specific code or form number | Poor (code has no semantic meaning) | Good (exact token match) |\n",
    "| Query is conceptual ('what is the leave policy') | Good | Medium |\n",
    "| Query is precise ('Form A-12 deadline') | Medium | Good |\n",
    "\n",
    "#### What Is Reciprocal Rank Fusion (RRF)?\n",
    "\n",
    "After dense and BM25 each produce a ranked list, we need to combine them into\n",
    "one final list.  **RRF** does this without needing to calibrate the raw scores\n",
    "(BM25 scores and cosine similarity are on completely different scales).\n",
    "\n",
    "RRF uses only the **rank position**, not the raw score:\n",
    "\n",
    "```\n",
    "RRF score for a chunk = 1 / (k + rank_in_dense_list)\n",
    "                      + 1 / (k + rank_in_bm25_list)\n",
    "\n",
    "where k = 60 (a constant that dampens the effect of very high ranks)\n",
    "\n",
    "Example (k=60):\n",
    "  Chunk A: rank 1 in dense, rank 1 in BM25\n",
    "    RRF = 1/(60+1) + 1/(60+1) = 0.0164 + 0.0164 = 0.0328  <- top score\n",
    "\n",
    "  Chunk B: rank 1 in dense only, not in BM25 top-5\n",
    "    RRF = 1/(60+1) + 0         = 0.0164 + 0     = 0.0164\n",
    "\n",
    "  Chunk C: rank 3 in dense, rank 2 in BM25\n",
    "    RRF = 1/(60+3) + 1/(60+2) = 0.0159 + 0.0161 = 0.0320\n",
    "```\n",
    "\n",
    "The chunk that ranks well in *both* lists rises to the top.  A chunk that only one\n",
    "system finds is not penalised \u2014 it still contributes its signal.\n",
    "\n",
    "The constant k=60 prevents rank 1 from dominating too heavily.  Without it,\n",
    "rank 1 (score=1.0) would be 10x rank 10 (score=0.1), which is too aggressive.\n",
    "With k=60, rank 1 (1/61=0.0164) is only ~1.1x rank 10 (1/70=0.0143).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4_vector_ref",
   "metadata": {},
   "source": [
    "### Two Retrieval Signals: Cosine Similarity (dense) and BM25 (keyword)\n",
    "\n",
    "**Dense retrieval \u2014 cosine similarity (Tutorial 1 baseline)**  \n",
    "Query and chunk texts are converted to embedding vectors; chunks are ranked by cosine similarity.\n",
    "Captures *semantic* similarity \u2014 works well even when the exact words differ.\n",
    "\n",
    "**BM25 keyword retrieval (new in this tutorial)**  \n",
    "Ranks chunks by term-frequency statistics.  \n",
    "Score \u221d how often query terms appear in the chunk, normalized by chunk length.  \n",
    "Captures *lexical* similarity \u2014 reliable for exact identifiers like `Form A-12`.\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF)**  \n",
    "Both ranked lists are merged: `score = 1/(k + rank_dense) + 1/(k + rank_bm25)`.  \n",
    "Chunks that rank well in *both* lists receive the highest combined score.\n",
    "\n",
    "See **Tutorial 1, cells 8\u201310** for the cosine similarity step-by-step walkthrough.",
    "\n",
    "#### Side-by-side toy example: how each signal produces its own top-k\n",
    "\n",
    "Both dense and keyword retrieval independently rank all chunks and return a top-k list.\n",
    "Reciprocal Rank Fusion (RRF) then combines the two ranked lists into a single one.\n",
    "\n",
    "```\n",
    "Query: 'Form A-12 reimbursement'\n",
    "\n",
    "Dense retrieval (cosine similarity)   BM25 keyword retrieval\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "rank 1 [0.88]  expense policy         rank 1 [9.2]  Form A-12 instructions   \u2190 exact match!\n",
    "rank 2 [0.81]  reimbursement guide    rank 2 [7.8]  reimbursement form guide\n",
    "rank 3 [0.74]  travel budget rules    rank 3 [5.1]  expense policy\n",
    "rank 4 [0.68]  form submission guide  rank 4 [3.4]  form submission guide\n",
    "rank 5 [0.61]  HR contact info        rank 5 [1.9]  travel budget rules\n",
    "\n",
    "Dense misses 'Form A-12 instructions' at rank 1 \u2014 the exact form name doesn't\n",
    "appear in its training data, so the embedding is not as close as BM25's match.\n",
    "\n",
    "RRF fusion (k=60 constant):  score = 1/(60+rank_dense) + 1/(60+rank_bm25)\n",
    "\n",
    " chunk                        RRF score  final rank\n",
    " \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    " Form A-12 instructions       0.01613    1   \u2190 keyword signal promoted it\n",
    " expense policy               0.01590    2\n",
    " reimbursement form guide     0.01573    3\n",
    " form submission guide        0.01524    4\n",
    " travel budget rules          0.01506    5\n",
    "```\n",
    "\n",
    "BM25's strong exact-match signal for 'Form A-12' lifts that chunk to the top even\n",
    "though dense retrieval ranked it only 1st \u2014 both happen to agree here, but in\n",
    "cases where they disagree (one scores high, the other low) RRF balances them.\n",
    "\n",
    "See **Tutorial 1 cells 10\u201313** for the nearest-neighbor + cosine similarity walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Implement retrieval functions and inspect dense vs keyword vs hybrid\n",
    "\n",
    "def dense_only(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "def keyword_only(question: str, top_k: int = 5):\n",
    "    return bm25_search(bm25_index, question, corpus, chunk_ids, top_k=top_k)\n",
    "\n",
    "def hybrid(question: str, top_k: int = 5):\n",
    "    return hybrid_retriever(question, top_k=top_k)\n",
    "\n",
    "probe = \"Do I need Form A-12 for my trip?\"\n",
    "\n",
    "dense_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(dense_only(probe))])\n",
    "keyword_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(keyword_only(probe))])\n",
    "hybrid_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(hybrid(probe))])\n",
    "\n",
    "print(\"Dense only\")\n",
    "display(dense_df)\n",
    "print(\"Keyword only\")\n",
    "display(keyword_df)\n",
    "print(\"Hybrid\")\n",
    "display(hybrid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a79250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8) Prompt assembly + end-to-end query\n",
    "\n",
    "def rag_answer_hybrid(question: str, top_k: int = 5):\n",
    "    retrieved = hybrid(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "answer, retrieved = rag_answer_hybrid(probe)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b985ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10) Evaluation + continuity summary table\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: hybrid(question, top_k=5),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 4 metrics:\", summarize(rows))\n",
    "\n",
    "continuity = pd.DataFrame(\n",
    "    [\n",
    "        {\"tutorial\": 1, \"change\": \"dense baseline with fixed chunks\"},\n",
    "        {\"tutorial\": 2, \"change\": \"semantic chunking\"},\n",
    "        {\"tutorial\": 3, \"change\": \"reranking\"},\n",
    "        {\"tutorial\": 4, \"change\": \"hybrid dense + keyword\"},\n",
    "    ]\n",
    ")\n",
    "continuity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}