{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b5c254",
   "metadata": {},
   "source": [
    "# Tutorial 4 â€” Hybrid Search (Dense + Keyword)\n",
    "\n",
    "This final tutorial adds BM25 keyword retrieval and fuses it with dense retrieval via Reciprocal Rank Fusion.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Q[Query] --> D[Dense Retriever]\n",
    "    Q --> K[Keyword Retriever BM25]\n",
    "    D --> F[RRF Fusion]\n",
    "    K --> F\n",
    "    F --> L[Top-k for Generation]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950df206",
   "metadata": {},
   "source": [
    "## Learning checkpoint: hybrid tradeoffs and readiness for benchmarking\n",
    "\n",
    "**What works better in Tutorial 4**\n",
    "- Keyword retrieval improves exact-token recall (e.g., `Form A-12`).\n",
    "- Dense retrieval still contributes semantic context.\n",
    "- Fusion provides more robust retrieval across question styles.\n",
    "\n",
    "**Challenges you should observe**\n",
    "- More moving parts increase tuning complexity (weights, top-k, fusion behavior).\n",
    "- Latency and system complexity are higher than baseline.\n",
    "- Hybrid is better in many cases, but not always dominant on every metric.\n",
    "\n",
    "**Why move to Tutorial 5**\n",
    "- At this point, intuition is not enough.\n",
    "- We benchmark all variants side-by-side to quantify quality, groundedness, and latency tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-5) Setup, handbook text, semantic chunks, embeddings, index\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever, build_hybrid_retriever\n",
    "from rag_tutorials.retrieval import build_bm25, bm25_search\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)\n",
    "chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "dense_retriever, _ = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial4_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "hybrid_retriever = build_hybrid_retriever(chunks, dense_retriever)\n",
    "\n",
    "bm25_index, corpus, chunk_ids = build_bm25(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Implement retrieval functions and inspect dense vs keyword vs hybrid\n",
    "\n",
    "def dense_only(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "def keyword_only(question: str, top_k: int = 5):\n",
    "    return bm25_search(bm25_index, question, corpus, chunk_ids, top_k=top_k)\n",
    "\n",
    "def hybrid(question: str, top_k: int = 5):\n",
    "    return hybrid_retriever(question, top_k=top_k)\n",
    "\n",
    "probe = \"Do I need Form A-12 for my trip?\"\n",
    "\n",
    "dense_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(dense_only(probe))])\n",
    "keyword_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(keyword_only(probe))])\n",
    "hybrid_df = pd.DataFrame([{\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:90]} for i, r in enumerate(hybrid(probe))])\n",
    "\n",
    "print(\"Dense only\")\n",
    "display(dense_df)\n",
    "print(\"Keyword only\")\n",
    "display(keyword_df)\n",
    "print(\"Hybrid\")\n",
    "display(hybrid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a79250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-8) Prompt assembly + end-to-end query\n",
    "\n",
    "def rag_answer_hybrid(question: str, top_k: int = 5):\n",
    "    retrieved = hybrid(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "answer, retrieved = rag_answer_hybrid(probe)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b985ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10) Evaluation + continuity summary table\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: hybrid(question, top_k=5),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 4 metrics:\", summarize(rows))\n",
    "\n",
    "continuity = pd.DataFrame(\n",
    "    [\n",
    "        {\"tutorial\": 1, \"change\": \"dense baseline with fixed chunks\"},\n",
    "        {\"tutorial\": 2, \"change\": \"semantic chunking\"},\n",
    "        {\"tutorial\": 3, \"change\": \"reranking\"},\n",
    "        {\"tutorial\": 4, \"change\": \"hybrid dense + keyword\"},\n",
    "    ]\n",
    ")\n",
    "continuity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
