{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a23c094",
   "metadata": {},
   "source": [
    "# Tutorial 2 — Semantic Chunking (Same Pipeline, Better Chunks)\n",
    "\n",
    "Only one variable changes from Tutorial 1: chunking strategy (`fixed` → `semantic`).\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Same Documents] --> B[Semantic Chunking]\n",
    "    B --> C[OpenAI Embeddings]\n",
    "    C --> D[Chroma]\n",
    "    E[Same Query Set] --> F[Dense Retrieval]\n",
    "    F --> G[Compare vs Tutorial 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355f2fe",
   "metadata": {},
   "source": [
    "## Learning checkpoint: what improved and what still fails\n",
    "\n",
    "**What works better in Tutorial 2**\n",
    "- Semantically grouped chunks preserve policy rule + condition together.\n",
    "- Recall on context-dependent questions should improve versus Tutorial 1.\n",
    "\n",
    "**Challenges you should observe**\n",
    "- Retrieval ranking can still surface a good chunk below weaker ones.\n",
    "- Similar chunks with overlapping terms may still be misordered.\n",
    "- Exact-token questions (e.g., specific form IDs) are not consistently top-ranked.\n",
    "\n",
    "**Why move to Tutorial 3**\n",
    "- Chunking is better now, but ranking quality is still a bottleneck.\n",
    "- We next add a reranking stage to reorder candidates by query-specific relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3) Setup, config, and load data\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure uv is available (installs with: pip install uv)\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "# Ensure notebook runs from repo root and local src/ is importable\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "]\n",
    "\n",
    "missing = [pkg for pkg in REQUIRED_PACKAGES if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "else:\n",
    "    print(\"All required packages are available.\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import fixed_chunk_documents, semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a44ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Chunk and normalize text: fixed vs semantic\n",
    "\n",
    "fixed_chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "semantic_chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"mode\": \"fixed\", \"count\": len(fixed_chunks), \"avg_chars\": sum(len(c.text) for c in fixed_chunks) / len(fixed_chunks)},\n",
    "    {\"mode\": \"semantic\", \"count\": len(semantic_chunks), \"avg_chars\": sum(len(c.text) for c in semantic_chunks) / len(semantic_chunks)},\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-8) Build embeddings/index and run retrieval pipeline on semantic chunks\n",
    "\n",
    "semantic_retriever, semantic_vectors = build_dense_retriever(\n",
    "    chunks=semantic_chunks,\n",
    "    collection_name=\"tutorial2_semantic_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "\n",
    "probe = \"What is the policy for working from another country?\"\n",
    "semantic_results = semantic_retriever(probe, top_k=5)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:110]}\n",
    "    for i, r in enumerate(semantic_results)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10) Evaluation and debug output (compareable with Tutorial 1)\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: semantic_retriever(question, top_k=5),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 2 metrics:\", summarize(rows))\n",
    "\n",
    "toy_q = queries[0].question\n",
    "toy_results = semantic_retriever(toy_q, top_k=5)\n",
    "print(\"\\nNovice trace for one query:\")\n",
    "for i, r in enumerate(toy_results, start=1):\n",
    "    print(f\"{i}. {r.chunk_id} | {r.score:.4f} | {r.text[:90]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
