{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a23c094",
   "metadata": {},
   "source": [
    "# Tutorial 2 \u2014 Semantic Chunking (Same Pipeline, Better Chunks)\n",
    "\n",
    "Only one variable changes from Tutorial 1: chunking strategy (`fixed` \u2192 `semantic`).\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Same Documents] --> B[Semantic Chunking]\n",
    "    B --> C[OpenAI Embeddings]\n",
    "    C --> D[Chroma]\n",
    "    E[Same Query Set] --> F[Dense Retrieval]\n",
    "    F --> G[Compare vs Tutorial 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355f2fe",
   "metadata": {},
   "source": [
    "## Learning checkpoint: what improved and what still fails\n",
    "\n",
    "**What works better in Tutorial 2**\n",
    "- Semantically grouped chunks preserve policy rule + condition together.\n",
    "- Recall on context-dependent questions should improve versus Tutorial 1.\n",
    "\n",
    "**Challenges you should observe**\n",
    "- Retrieval ranking can still surface a good chunk below weaker ones.\n",
    "- Similar chunks with overlapping terms may still be misordered.\n",
    "- Exact-token questions (e.g., specific form IDs) are not consistently top-ranked.\n",
    "\n",
    "**Why move to Tutorial 3**\n",
    "- Chunking is better now, but ranking quality is still a bottleneck.\n",
    "- We next add a reranking stage to reorder candidates by query-specific relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294f2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing packages: ['rank_bm25']\n",
      "Running: uv sync\n",
      "Installing into current kernel with pip: ['rank-bm25']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m204 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m180 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from rank-bm25) (2.3.1)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1-3) Setup, config, and load data\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure uv is available (installs with: pip install uv)\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "# Ensure notebook runs from repo root and local src/ is importable\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "]\n",
    "PIP_NAME_MAP = {\"rank_bm25\": \"rank-bm25\", \"sentence_transformers\": \"sentence-transformers\", \"dotenv\": \"python-dotenv\"}\n",
    "\n",
    "def find_missing(packages: list[str]) -> list[str]:\n",
    "    importlib.invalidate_caches()\n",
    "    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    print(\"Installing into current kernel with pip:\", pip_targets)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing in current kernel: {final_missing}\")\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "from rag_tutorials.chunking import fixed_chunk_documents, semantic_chunk_documents\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is required\")\n",
    "\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "chat_model = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "handbook_path = Path(\"data/handbook_manual.txt\")\n",
    "queries_path = Path(\"data/queries.jsonl\")\n",
    "if not handbook_path.exists() or not queries_path.exists():\n",
    "    raise FileNotFoundError(\"Run: uv run python scripts/generate_data.py\")\n",
    "\n",
    "documents = load_handbook_documents(handbook_path)\n",
    "queries = load_queries(queries_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a44ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed</td>\n",
       "      <td>6</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>semantic</td>\n",
       "      <td>7</td>\n",
       "      <td>163.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mode  count   avg_chars\n",
       "0     fixed      6  191.000000\n",
       "1  semantic      7  163.142857"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Chunk and normalize text: fixed vs semantic\n",
    "\n",
    "fixed_chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "semantic_chunks = semantic_chunk_documents(documents)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"mode\": \"fixed\", \"count\": len(fixed_chunks), \"avg_chars\": sum(len(c.text) for c in fixed_chunks) / len(fixed_chunks)},\n",
    "    {\"mode\": \"semantic\", \"count\": len(semantic_chunks), \"avg_chars\": sum(len(c.text) for c in semantic_chunks) / len(semantic_chunks)},\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037c0187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: International Work\n",
      "\n",
      "Fixed chunks:\n",
      "[1] Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, e\n",
      "[2] mployees must open a Global Mobility case and obtain HR, Legal, and Payroll approval. Violations can trigger immigration\n",
      "[3] , payroll, and tax exposure.\n",
      "\n",
      "Semantic chunks:\n",
      "[1] Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, employees must open a Global Mobility case and obtain HR, Legal, and Payroll approval\n",
      "[2] Violations can trigger immigration, payroll, and tax exposure.\n"
     ]
    }
   ],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t2_vector_ref",
   "metadata": {},
   "source": [
    "### Vector Embedding and Nearest-Neighbor Search (same mechanism as Tutorial 1, different chunks)\n",
    "\n",
    "Each chunk is embedded into a high-dimensional vector using `text-embedding-3-small`.\n",
    "Retrieval ranks chunks by **cosine similarity** between the query vector and every chunk\n",
    "vector, then returns the **top-k highest-scoring** chunks \u2014 this is nearest-neighbor search.\n",
    "\n",
    "#### Quick recap: how top-k nearest-neighbor works\n",
    "\n",
    "```\n",
    "for each chunk in the index:\n",
    "    score = cosine_similarity(query_vector, chunk_vector)\n",
    "sort chunks by score descending\n",
    "return first k chunks           \u2190 these are the k nearest neighbors\n",
    "```\n",
    "\n",
    "#### What changes in Tutorial 2: the chunks, not the search algorithm\n",
    "\n",
    "The nearest-neighbor algorithm is **identical** to Tutorial 1.  The only difference is\n",
    "*what* is stored in the index.  Semantic chunking groups sentences by meaning, so each\n",
    "chunk vector captures a tighter, more coherent idea \u2014 this shifts which chunk ends up\n",
    "as the nearest neighbor for a given query.\n",
    "\n",
    "```\n",
    "Same query: 'working from abroad'\n",
    "\n",
    "Tutorial 1 (fixed chunks)         Tutorial 2 (semantic chunks)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500         \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "rank 1  [0.82]  remote-work-p1   rank 1  [0.91]  full remote-work section\n",
    "rank 2  [0.74]  remote-work-p2   rank 2  [0.78]  international-transfer block\n",
    "rank 3  [0.61]  leave-general    rank 3  [0.65]  tax-compliance paragraph\n",
    "\n",
    "Fixed chunks split mid-sentence \u2192 two low-quality neighbors.\n",
    "Semantic chunks keep the policy together \u2192 one high-quality neighbor.\n",
    "```\n",
    "\n",
    "See **Tutorial 1 cells 10\u201313** for the full cosine-similarity derivation and\n",
    "a step-by-step nearest-neighbor example with 6 toy chunk vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c56a51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>score</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-SEM-00</td>\n",
       "      <td>0.160670</td>\n",
       "      <td>Working from another country is capped at 14 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>DOC-HB-INTERNATIONALTAX-SEM-00</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>Employees traveling internationally may need F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DOC-HB-REMOTEWORK-SEM-00</td>\n",
       "      <td>-0.191435</td>\n",
       "      <td>Z-Tech encourages remote work from home, co-wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DOC-HB-TRAVELAPPROVAL-SEM-00</td>\n",
       "      <td>-0.220997</td>\n",
       "      <td>International travel requests must be submitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>DOC-HB-SECURITY-SEM-00</td>\n",
       "      <td>-0.323462</td>\n",
       "      <td>Employees handling customer data while traveli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                         chunk_id     score  \\\n",
       "0     1  DOC-HB-INTERNATIONALWORK-SEM-00  0.160670   \n",
       "1     2   DOC-HB-INTERNATIONALTAX-SEM-00 -0.017738   \n",
       "2     3         DOC-HB-REMOTEWORK-SEM-00 -0.191435   \n",
       "3     4     DOC-HB-TRAVELAPPROVAL-SEM-00 -0.220997   \n",
       "4     5           DOC-HB-SECURITY-SEM-00 -0.323462   \n",
       "\n",
       "                                             preview  \n",
       "0  Working from another country is capped at 14 d...  \n",
       "1  Employees traveling internationally may need F...  \n",
       "2  Z-Tech encourages remote work from home, co-wo...  \n",
       "3  International travel requests must be submitte...  \n",
       "4  Employees handling customer data while traveli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-8) Build embeddings/index and run retrieval pipeline on semantic chunks\n",
    "\n",
    "semantic_retriever, semantic_vectors = build_dense_retriever(\n",
    "    chunks=semantic_chunks,\n",
    "    collection_name=\"tutorial2_semantic_dense\",\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "\n",
    "probe = \"What is the policy for working from another country?\"\n",
    "semantic_results = semantic_retriever(probe, top_k=5)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"rank\": i + 1, \"chunk_id\": r.chunk_id, \"score\": r.score, \"preview\": r.text[:110]}\n",
    "    for i, r in enumerate(semantic_results)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ef358c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutorial 2 metrics: {'recall_at_k': 1.0, 'mrr': 0.975, 'latency_ms': 2040.9615729935467, 'groundedness': 0.7643919249916471}\n",
      "\n",
      "Novice trace for one query:\n",
      "1. DOC-HB-REMOTEWORK-SEM-00 | 0.1225 | Z-Tech encourages remote work from home, co-working spaces, or temporary domestic location\n",
      "2. DOC-HB-INTERNATIONALWORK-SEM-00 | -0.2440 | Working from another country is capped at 14 days in a rolling 12-month period without per\n",
      "3. DOC-HB-REMOTEWORK-SEM-01 | -0.3477 | Public Wi-Fi usage is allowed only with corporate VPN enabled.\n",
      "4. DOC-HB-INTERNATIONALTAX-SEM-00 | -0.3520 | Employees traveling internationally may need Form A-12 before departure when cross-border \n",
      "5. DOC-HB-SECURITY-SEM-00 | -0.3534 | Employees handling customer data while traveling must use VPN, hardware-backed MFA, and en\n"
     ]
    }
   ],
   "source": [
    "# 9-10) Evaluation and debug output (compareable with Tutorial 1)\n",
    "\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: semantic_retriever(question, top_k=5),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=chat_model),\n",
    "        top_k=5,\n",
    "    )\n",
    "    for q in queries[:20]\n",
    "]\n",
    "\n",
    "print(\"Tutorial 2 metrics:\", summarize(rows))\n",
    "\n",
    "toy_q = queries[0].question\n",
    "toy_results = semantic_retriever(toy_q, top_k=5)\n",
    "print(\"\\nNovice trace for one query:\")\n",
    "for i, r in enumerate(toy_results, start=1):\n",
    "    print(f\"{i}. {r.chunk_id} | {r.score:.4f} | {r.text[:90]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}