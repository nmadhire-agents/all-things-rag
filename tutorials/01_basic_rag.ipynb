{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622af14c",
   "metadata": {},
   "source": [
    "# Tutorial 1 \u2014 Basic RAG (Dense Retrieval Baseline)\n",
    "\n",
    "Welcome!  This is the **starting point** of a five-part series that builds a\n",
    "complete Retrieval-Augmented Generation (RAG) system step by step.\n",
    "Every tutorial changes exactly **one thing** so you can see the impact clearly.\n",
    "\n",
    "## Your Learning Roadmap\n",
    "\n",
    "```\n",
    " Tutorial 1 \u2500\u2500\u25ba Tutorial 2 \u2500\u2500\u25ba Tutorial 3 \u2500\u2500\u25ba Tutorial 4 \u2500\u2500\u25ba Tutorial 5\n",
    " Basic RAG      Better         Add            Add Keyword    Benchmark\n",
    " (you are       Chunking       Reranking      Search         All Four\n",
    "  here)         (T2)           (T3)           (T4)           (T5)\n",
    "```\n",
    "\n",
    "**What you will build in this tutorial:**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Documents] --> B[Fixed Chunking]\n",
    "    B --> C[OpenAI Embeddings]\n",
    "    C --> D[Chroma Vector Index]\n",
    "    E[User Query] --> F[Query Embedding]\n",
    "    F --> D\n",
    "    D --> G[Top-k Chunks]\n",
    "    G --> H[LLM Answer]\n",
    "```\n",
    "\n",
    "**By the end of this notebook you will understand:**\n",
    "- What RAG is and *why* it exists\n",
    "- What a token is and why documents must be chunked\n",
    "- What an embedding vector is and how cosine similarity works\n",
    "- How nearest-neighbor search finds the most relevant chunks\n",
    "- How to evaluate a RAG pipeline with Recall, MRR, Groundedness, and Latency\n",
    "\n",
    "**Prerequisites:** Python basics, a curiosity for how AI systems work \u2014 no ML background needed.\n",
    "\n",
    "Continuity note:\n",
    "- Tutorial 2 keeps the same pipeline but changes **chunking**.\n",
    "- Tutorial 3 keeps T2 and adds a **reranker**.\n",
    "- Tutorial 4 keeps T3 and adds **keyword (BM25) retrieval**.\n",
    "- Tutorial 5 benchmarks all four under identical conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what_is_rag",
   "metadata": {},
   "source": [
    "## What is RAG and Why Does It Exist?\n",
    "\n",
    "### The Problem: LLMs Know a Lot, But Not Everything\n",
    "\n",
    "**Large Language Models (LLMs)** \u2014 such as GPT-4 \u2014 are AI systems trained on enormous\n",
    "amounts of text (books, websites, code).  They learn patterns in language so well that\n",
    "they can answer questions, write essays, and summarise documents.\n",
    "\n",
    "But they have three hard limitations:\n",
    "\n",
    "| Limitation | Plain-English Meaning | Example |\n",
    "|------------|----------------------|--------|\n",
    "| **Knowledge cutoff** | Trained up to a fixed date; knows nothing newer | GPT-4 won't know about a policy updated last month |\n",
    "| **Private data blindspot** | Has never seen your internal documents | Your employee handbook, contracts, wiki |\n",
    "| **Hallucination** | Can invent plausible-sounding but wrong answers | Confidently states the wrong leave entitlement |\n",
    "\n",
    "### The Solution: Give the LLM the Right Context at Query Time\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** fixes all three problems by:\n",
    "\n",
    "1. **Storing your documents** in a searchable index.\n",
    "2. **Finding the relevant passages** when a user asks a question.\n",
    "3. **Injecting those passages** into the LLM's prompt so it answers from *your* data.\n",
    "\n",
    "Think of it like an open-book exam:\n",
    "```\n",
    "Without RAG:  Student answers from memory \u2192 may be wrong or outdated\n",
    "With RAG:     Student looks up the answer in the textbook \u2192 grounded in fact\n",
    "```\n",
    "\n",
    "### The RAG Pipeline in Plain English\n",
    "\n",
    "```\n",
    "INDEXING (done once, ahead of time)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "  1. Take your documents (e.g. handbook_manual.txt)\n",
    "  2. Split into small passages called chunks\n",
    "  3. Convert each chunk to a numeric vector (embedding)\n",
    "  4. Store those vectors in a vector database (Chroma)\n",
    "\n",
    "QUERYING (done every time a user asks a question)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "  5. Convert the user's question to a vector\n",
    "  6. Find the chunks whose vectors are closest to the question vector\n",
    "  7. Build a prompt: \"Answer this question using these passages: ...\"\n",
    "  8. Send prompt to the LLM \u2192 get a grounded answer\n",
    "```\n",
    "\n",
    "> **Why does this work?**  If two pieces of text mean similar things, their vectors\n",
    "> point in similar directions.  So \"What is the remote work policy?\" and\n",
    "> \"Employees may work remotely if...\" will produce vectors that are close together \u2014\n",
    "> even though they share no exact words.  This is the magic of embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed2d17",
   "metadata": {},
   "source": [
    "## Learning checkpoint: what works vs what breaks\n",
    "\n",
    "**What works in Tutorial 1**\n",
    "- Dense retrieval can find generally related handbook content.\n",
    "- End-to-end RAG flow is functional (ingest \u2192 chunk \u2192 embed \u2192 retrieve \u2192 answer).\n",
    "\n",
    "**Challenges you should observe**\n",
    "- Query intent can be too broad for nearest-neighbor retrieval.\n",
    "- Exception-heavy policy questions may return partially relevant chunks.\n",
    "- Exact policy identifiers (like forms/codes) are often weakly handled.\n",
    "\n",
    "**Why move to Tutorial 2**\n",
    "- The first bottleneck is chunk quality.\n",
    "- We next improve *how text is split* so policy context stays intact before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003b42c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are available.\n",
      "Python: 3.11.13\n",
      "Working directory: /Users/avy/GitHubProjects/allagents/all-things-rag\n",
      "Repo root: /Users/avy/GitHubProjects/allagents/all-things-rag\n",
      "Using src path: /Users/avy/GitHubProjects/allagents/all-things-rag/src\n"
     ]
    }
   ],
   "source": [
    "# 1) Set Up Environment and Dependencies\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure uv is available (installs with: pip install uv)\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "# Ensure notebook runs from repo root and local src/ is importable\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "]\n",
    "\n",
    "PIP_NAME_MAP = {\n",
    "    \"rank_bm25\": \"rank-bm25\",\n",
    "    \"sentence_transformers\": \"sentence-transformers\",\n",
    "    \"dotenv\": \"python-dotenv\",\n",
    "}\n",
    "\n",
    "\n",
    "def find_missing(packages: list[str]) -> list[str]:\n",
    "    \"\"\"Return package import names not available in current kernel.\"\"\"\n",
    "    importlib.invalidate_caches()\n",
    "    return [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "\n",
    "missing = find_missing(REQUIRED_PACKAGES)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "\n",
    "missing_after_sync = find_missing(REQUIRED_PACKAGES)\n",
    "if missing_after_sync:\n",
    "    print(\"Still missing in active kernel after uv sync:\", missing_after_sync)\n",
    "    pip_targets = [PIP_NAME_MAP.get(pkg, pkg) for pkg in missing_after_sync]\n",
    "    print(\"Installing into current kernel with pip:\", pip_targets)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", *pip_targets], check=True)\n",
    "\n",
    "final_missing = find_missing(REQUIRED_PACKAGES)\n",
    "if final_missing:\n",
    "    raise ImportError(f\"Dependencies still missing in current kernel: {final_missing}\")\n",
    "\n",
    "print(\"All required packages are available.\")\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"Using src path:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84c12bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(embedding_model='text-embedding-3-small', chat_model='gpt-4.1-mini', chunk_mode='fixed', top_k=5, sample_eval_size=20, handbook_path='data/handbook_manual.txt', queries_path='data/queries.jsonl')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Define Configuration and Paths\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    embedding_model: str = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    chat_model: str = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "    chunk_mode: str = \"fixed\"\n",
    "    top_k: int = 5\n",
    "    sample_eval_size: int = 20\n",
    "    handbook_path: str = \"data/handbook_manual.txt\"\n",
    "    queries_path: str = \"data/queries.jsonl\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Copy .env.example to .env and set your key.\")\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d81fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text: data/handbook_manual.txt\n",
      "Parsed handbook sections: 5\n",
      "Queries: 200\n",
      "Sample parsed document: Document(doc_id='DOC-HB-REMOTEWORK', title='Z-Tech Handbook - Remote Work', section='Remote Work', text='Z-Tech encourages remote work from home, co-working spaces, or temporary domestic locations. Employees must stay reachable during assigned timezone hours and use approved managed devices. Public Wi-Fi usage is allowed only with corporate VPN enabled. Employees are responsible for confirming local workspace privacy when joining meetings that include customer data or personnel topics. Calendar availability must reflect working blocks, breaks, and approved out-of-office windows so cross-functional teams can plan handoffs. Managers may define team-specific overlap hours when projects involve coordination across offices in different time zones. Home-office expenses are reimbursable only for pre-approved categories listed in the internal procurement guide. Employees should review ergonomic setup guidance quarterly and complete the annual safety attestation in the HR portal. Temporary domestic work from a location outside the home office state may require payroll location review if extended beyond 30 days. Use of personal devices for source-code access is prohibited unless enrolled in mobile device management and approved by security. Teams handling regulated datasets must use approved screen-sharing controls that prevent accidental exposure during demos.')\n"
     ]
    }
   ],
   "source": [
    "# 3) Load and Normalize Source Documents (shared handbook text + query set)\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "\n",
    "if not Path(cfg.handbook_path).exists() or not Path(cfg.queries_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Shared data files are missing. Run: uv run python scripts/generate_data.py\"\n",
    "    )\n",
    "\n",
    "documents = load_handbook_documents(cfg.handbook_path)\n",
    "queries = load_queries(cfg.queries_path)\n",
    "\n",
    "print(\"Source text:\", cfg.handbook_path)\n",
    "print(\"Parsed handbook sections:\", len(documents))\n",
    "print(\"Queries:\", len(queries))\n",
    "print(\"Sample parsed document:\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why_chunk",
   "metadata": {},
   "source": [
    "### Why Do We Need to Chunk Documents?\n",
    "\n",
    "Before we can embed and search our documents, we must split them into smaller pieces\n",
    "called **chunks**.  Here is why \u2014 and what a token is.\n",
    "\n",
    "#### What Is a Token?\n",
    "\n",
    "A **token** is the basic unit of text that an LLM reads.  It is *roughly* equal to\n",
    "three-quarters of a word in English.\n",
    "\n",
    "```\n",
    "\"Hello, world!\"      \u2192  3 tokens   [\"Hello\", \",\", \" world!\"]\n",
    "\"international\"      \u2192  3 tokens   [\"intern\", \"ation\", \"al\"]\n",
    "\"I like dogs.\"       \u2192  4 tokens   [\"I\", \" like\", \" dogs\", \".\"]\n",
    "```\n",
    "\n",
    "An average page of text \u2248 500 tokens.  Most LLMs have a **context window** of\n",
    "4,000\u2013128,000 tokens \u2014 the maximum amount of text they can read *in one go*.\n",
    "\n",
    "#### Three Reasons We Chunk\n",
    "\n",
    "1. **Context window limits** \u2014 LLMs can only read a fixed number of tokens at once.\n",
    "   Feeding an entire document would overflow the limit and increase cost significantly.\n",
    "\n",
    "2. **Retrieval precision** \u2014 A chunk captures one *specific idea*.\n",
    "   If we embedded entire sections, the resulting vector would average out many ideas,\n",
    "   making it harder to find the exact passage that answers a question.\n",
    "\n",
    "   ```\n",
    "   Section vector  \u2248 average of (remote work + leave + expenses + ...)\n",
    "   Chunk vector    \u2248 just remote work policy\n",
    "   \u2192 Query \"remote work\" matches the chunk much better\n",
    "   ```\n",
    "\n",
    "3. **Cost** \u2014 Embedding and searching smaller units is cheaper and faster.\n",
    "\n",
    "#### Fixed vs Semantic Chunking (preview)\n",
    "\n",
    "| Strategy | How it splits | Problem |\n",
    "|----------|--------------|--------|\n",
    "| **Fixed** (this tutorial) | Every N tokens, regardless of meaning | Can split a sentence mid-thought |\n",
    "| **Semantic** (Tutorial 2) | At natural sentence/topic boundaries | Preserves meaning \u2014 better retrieval |\n",
    "\n",
    "```\n",
    "Original text:\n",
    "  \"...Employees may work remotely for up to 90 days per year.\n",
    "   A manager approval is required before the period starts...\"\n",
    "\n",
    "Fixed chunking (260 tokens) might split here:\n",
    "  Chunk A: \"...Employees may work remotely for up to 90 days per year.\n",
    "            A manager approval is\"             \u2190 sentence cut off!\n",
    "  Chunk B: \"required before the period starts...\"\n",
    "\n",
    "Semantic chunking keeps the whole rule together in one chunk \u2014 Tutorial 2 shows why this matters.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac16d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           chunk_id                    doc_id  \\\n",
      "0          DOC-HB-REMOTEWORK-FIX-00         DOC-HB-REMOTEWORK   \n",
      "1          DOC-HB-REMOTEWORK-FIX-01         DOC-HB-REMOTEWORK   \n",
      "2          DOC-HB-REMOTEWORK-FIX-02         DOC-HB-REMOTEWORK   \n",
      "3          DOC-HB-REMOTEWORK-FIX-03         DOC-HB-REMOTEWORK   \n",
      "4          DOC-HB-REMOTEWORK-FIX-04         DOC-HB-REMOTEWORK   \n",
      "5   DOC-HB-INTERNATIONALWORK-FIX-00  DOC-HB-INTERNATIONALWORK   \n",
      "6   DOC-HB-INTERNATIONALWORK-FIX-01  DOC-HB-INTERNATIONALWORK   \n",
      "7   DOC-HB-INTERNATIONALWORK-FIX-02  DOC-HB-INTERNATIONALWORK   \n",
      "8   DOC-HB-INTERNATIONALWORK-FIX-03  DOC-HB-INTERNATIONALWORK   \n",
      "9   DOC-HB-INTERNATIONALWORK-FIX-04  DOC-HB-INTERNATIONALWORK   \n",
      "10   DOC-HB-INTERNATIONALTAX-FIX-00   DOC-HB-INTERNATIONALTAX   \n",
      "11   DOC-HB-INTERNATIONALTAX-FIX-01   DOC-HB-INTERNATIONALTAX   \n",
      "12   DOC-HB-INTERNATIONALTAX-FIX-02   DOC-HB-INTERNATIONALTAX   \n",
      "13   DOC-HB-INTERNATIONALTAX-FIX-03   DOC-HB-INTERNATIONALTAX   \n",
      "14   DOC-HB-INTERNATIONALTAX-FIX-04   DOC-HB-INTERNATIONALTAX   \n",
      "15     DOC-HB-TRAVELAPPROVAL-FIX-00     DOC-HB-TRAVELAPPROVAL   \n",
      "16     DOC-HB-TRAVELAPPROVAL-FIX-01     DOC-HB-TRAVELAPPROVAL   \n",
      "17     DOC-HB-TRAVELAPPROVAL-FIX-02     DOC-HB-TRAVELAPPROVAL   \n",
      "18     DOC-HB-TRAVELAPPROVAL-FIX-03     DOC-HB-TRAVELAPPROVAL   \n",
      "19     DOC-HB-TRAVELAPPROVAL-FIX-04     DOC-HB-TRAVELAPPROVAL   \n",
      "20           DOC-HB-SECURITY-FIX-00           DOC-HB-SECURITY   \n",
      "21           DOC-HB-SECURITY-FIX-01           DOC-HB-SECURITY   \n",
      "22           DOC-HB-SECURITY-FIX-02           DOC-HB-SECURITY   \n",
      "23           DOC-HB-SECURITY-FIX-03           DOC-HB-SECURITY   \n",
      "\n",
      "               section                                               text  \n",
      "0          Remote Work  Z-Tech encourages remote work from home, co-wo...  \n",
      "1          Remote Work   are responsible for confirming local workspac...  \n",
      "2          Remote Work  ffs. Managers may define team-specific overlap...  \n",
      "3          Remote Work  view ergonomic setup guidance quarterly and co...  \n",
      "4          Remote Work  or source-code access is prohibited unless enr...  \n",
      "5   International Work  Working from another country is capped at 14 d...  \n",
      "6   International Work  xposure. Employees must submit destination cou...  \n",
      "7   International Work  ome countries require pre-travel right-to-work...  \n",
      "8   International Work  mulate toward compliance thresholds and trigge...  \n",
      "9   International Work  f approvals are denied or delayed near departu...  \n",
      "10   International Tax  Employees traveling internationally may need F...  \n",
      "11   International Tax   include expected activities, legal entity ser...  \n",
      "12   International Tax  l estimate, employees must submit an updated F...  \n",
      "13   International Tax  and work location for annual compliance audits...  \n",
      "14   International Tax  does not imply approval in another. Failure to...  \n",
      "15     Travel Approval  International travel requests must be submitte...  \n",
      "16     Travel Approval  n, estimated budget, and expected customer or ...  \n",
      "17     Travel Approval  iming exception. Hotel selections must align w...  \n",
      "18     Travel Approval  equired, budget owners must confirm cost cente...  \n",
      "19     Travel Approval   emergency contacts and destination-specific s...  \n",
      "20            Security  Employees handling customer data while traveli...  \n",
      "21            Security  pter is used. Sensitive files should remain in...  \n",
      "22            Security  ctions performed outside trusted office networ...  \n",
      "23            Security  vailable. Incident reports must include locati...  \n"
     ]
    }
   ],
   "source": [
    "# 4) Split Documents into Chunks (fixed chunking baseline)\n",
    "\n",
    "from dataclasses import asdict\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "import pandas as pd\n",
    "\n",
    "chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "\n",
    "chunk_df = pd.DataFrame([asdict(c) for c in chunks])\n",
    "stats = {\n",
    "    \"chunk_count\": len(chunk_df),\n",
    "    \"avg_chunk_chars\": chunk_df.text.map(len).mean(),\n",
    "    \"max_chunk_chars\": chunk_df.text.map(len).max(),\n",
    "}\n",
    "#print(stats)\n",
    "print(chunk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f14881ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: International Work\n",
      "\n",
      "Fixed chunks:\n",
      "[1] Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, e\n",
      "[2] mployees must open a Global Mobility case and obtain HR, Legal, and Payroll approval. Violations can trigger immigration\n",
      "[3] , payroll, and tax exposure. Employees must submit destination country, travel dates, host entity, and work purpose when\n",
      "[4]  opening the Global Mobility case. Approval decisions depend on role type, customer access level, and whether on-site ac\n",
      "[5] tivities include contract negotiation. Some countries require pre-travel right-to-work checks even for short stays under\n",
      "[6]  the 14-day cap. International work days are counted using local calendar dates at destination, not departure timezone t\n",
      "[7] imestamps. Repeated short trips to the same country can accumulate toward compliance thresholds and trigger additional r\n",
      "[8] eview. Employees are responsible for carrying supporting approval documents while traveling in case local authorities re\n",
      "[9] quest evidence. Managers should verify project plans include fallback coverage if approvals are denied or delayed near d\n",
      "[10] eparture dates. Cross-border access to production systems may be limited by data residency rules, and engineering except\n",
      "[11] ions require sign-off.\n",
      "\n",
      "Semantic chunks:\n",
      "[1] Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, employees must open a Global Mobility case and obtain HR, Legal, and Payroll approval\n",
      "[2] Violations can trigger immigration, payroll, and tax exposure. Employees must submit destination country, travel dates, host entity, and work purpose when opening the Global Mobility case. Approval decisions depend on role type, customer access level, and whether on-site activities include contract negotiation. Some countries require pre-travel right-to-work checks even for short stays under the 14-day cap. International work days are counted using local calendar dates at destination, not departure timezone timestamps. Repeated short trips to the same country can accumulate toward compliance thresholds and trigger additional review. Employees are responsible for carrying supporting approval documents while traveling in case local authorities request evidence. Managers should verify project plans include fallback coverage if approvals are denied or delayed near departure dates. Cross-border access to production systems may be limited by data residency rules, and engineering exceptions require sign-off.\n"
     ]
    }
   ],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9ec69",
   "metadata": {},
   "source": [
    "## Novice Lens: How Embeddings and Retrieval Actually Work\n",
    "\n",
    "This section slows down and walks through every step with concrete numbers.\n",
    "If you are new to machine learning, read this before running the retrieval code.\n",
    "\n",
    "### The Big Picture (sequence diagram)\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User Query\n",
    "    participant E as Embedding Model\n",
    "    participant V as Vector Store\n",
    "    participant L as LLM\n",
    "    U->>E: \"working from another country\"\n",
    "    Note over E: Converts text to a list of 1536 numbers\n",
    "    E->>V: query vector [0.12, -0.34, 0.87, ...]\n",
    "    Note over V: Compares query vector to all stored chunk vectors\n",
    "    V-->>U: top-k chunks + similarity scores\n",
    "    U->>L: question + retrieved chunks (as context)\n",
    "    Note over L: Reads context, generates grounded answer\n",
    "    L-->>U: \"Employees may work remotely for up to 90 days...\"\n",
    "```\n",
    "\n",
    "We will inspect each of these steps with real numbers below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector_basics_explainer",
   "metadata": {},
   "source": [
    "### What Is an Embedding Vector?\n",
    "\n",
    "An **embedding** is a list of floating-point numbers that represents the *meaning*\n",
    "of a piece of text.  It is produced by a neural network (the \"embedding model\").\n",
    "\n",
    "#### Analogy: colour as a 3-number vector\n",
    "\n",
    "You already know one type of vector: an RGB colour.\n",
    "\n",
    "```\n",
    "Red:   [255,   0,   0]   \u2190 3 numbers\n",
    "Blue:  [  0,   0, 255]\n",
    "Pink:  [255, 150, 150]   \u2190 close to Red \u2014 similar meaning!\n",
    "```\n",
    "\n",
    "A text embedding works the same way, but instead of 3 numbers describing colour,\n",
    "we use **1536 numbers** describing meaning.  Words or sentences with similar meaning\n",
    "produce vectors that are numerically close.\n",
    "\n",
    "```\n",
    "\"remote work policy\"      \u2192 [0.80, 0.20, 0.50, ...]   (1536 numbers)\n",
    "\"working from abroad\"     \u2192 [0.85, 0.15, 0.45, ...]   \u2190 very similar!\n",
    "\"annual leave rules\"      \u2192 [0.10, 0.90, 0.30, ...]   \u2190 very different\n",
    "```\n",
    "\n",
    "#### Why 1536 dimensions?\n",
    "\n",
    "More dimensions = more capacity to encode nuance.  `text-embedding-3-small` uses 1536\n",
    "because the model learned that many independent \"aspects\" of meaning from its training data.\n",
    "You never see all 1536 values directly \u2014 the embedding model handles this internally.\n",
    "\n",
    "---\n",
    "\n",
    "### How Do We Measure Similarity? \u2014 Cosine Similarity\n",
    "\n",
    "Once we have vectors, we need a way to ask: *how similar are two vectors?*\n",
    "\n",
    "We use **cosine similarity** \u2014 it measures the angle between two vectors.  Think of\n",
    "each vector as an arrow pointing from the origin in high-dimensional space:\n",
    "\n",
    "```\n",
    "Two arrows pointing in the same direction \u2192 angle \u2248 0\u00b0 \u2192 cosine \u2248 1.0  (very similar)\n",
    "Two arrows perpendicular                  \u2192 angle = 90\u00b0 \u2192 cosine = 0.0  (unrelated)\n",
    "Two arrows pointing opposite              \u2192 angle = 180\u00b0 \u2192 cosine = -1.0 (opposite)\n",
    "```\n",
    "\n",
    "#### The Formula (and what each part means)\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\cdot \\|\\mathbf{B}\\|}$$\n",
    "\n",
    "| Symbol | Name | Plain English |\n",
    "|--------|------|---------------|\n",
    "| $\\mathbf{A} \\cdot \\mathbf{B}$ | **Dot product** | Multiply each pair of matching numbers, then add them all up |\n",
    "| $\\|\\mathbf{A}\\|$ | **Norm** (magnitude) | The \"length\" of vector A \u2014 square root of sum of squares |\n",
    "| Dividing by norms | **Normalisation** | Removes the effect of vector length so only direction matters |\n",
    "\n",
    "**Dot product example (3-dim):**\n",
    "```\n",
    "A = [0.80, 0.20, 0.50]\n",
    "B = [0.85, 0.15, 0.45]\n",
    "\n",
    "A\u00b7B = (0.80\u00d70.85) + (0.20\u00d70.15) + (0.50\u00d70.45)\n",
    "    =   0.680     +   0.030     +   0.225\n",
    "    =   0.935\n",
    "```\n",
    "\n",
    "**Norm example:**\n",
    "```\n",
    "||A|| = sqrt(0.80\u00b2 + 0.20\u00b2 + 0.50\u00b2)\n",
    "      = sqrt(0.640 + 0.040 + 0.250)\n",
    "      = sqrt(0.930)\n",
    "      = 0.964\n",
    "```\n",
    "\n",
    "| Cosine Score | Meaning |\n",
    "|:---:|---|\n",
    "| **1.0** | Identical direction \u2014 same meaning |\n",
    "| **0.8\u20130.99** | Very similar |\n",
    "| **0.5\u20130.79** | Moderately similar |\n",
    "| **0.0\u20130.49** | Low similarity |\n",
    "| **< 0** | Opposite meaning (rare for text) |\n",
    "\n",
    "The code cell below walks through every arithmetic step with toy vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosine_similarity_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector and cosine similarity walkthrough \u2014 toy 3-dimensional example\n",
    "# (Real OpenAI embeddings use 1536 dims; the math is identical)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Toy vectors representing meaning in 3-dimensional space\n",
    "vec_remote_work   = np.array([0.80, 0.20, 0.50])  # 'remote work policy'\n",
    "vec_leave_policy  = np.array([0.10, 0.90, 0.30])  # 'annual leave rules'\n",
    "vec_international_transfer = np.array([0.75, 0.25, 0.55])  # 'international work transfer'\n",
    "\n",
    "query_vec = np.array([0.85, 0.15, 0.45])          # query: 'working from abroad'\n",
    "\n",
    "print(\"Query vector:          \", query_vec)\n",
    "print(\"'remote work' vector:  \", vec_remote_work)\n",
    "print(\"'leave policy' vector: \", vec_leave_policy)\n",
    "print(\"'international transfer' vector:\", vec_international_transfer)\n",
    "print()\n",
    "\n",
    "# ---- Step-by-step cosine similarity: query vs 'remote work' ----\n",
    "dot_product    = np.dot(query_vec, vec_remote_work)\n",
    "norm_query     = np.linalg.norm(query_vec)\n",
    "norm_remote    = np.linalg.norm(vec_remote_work)\n",
    "cosine_score   = dot_product / (norm_query * norm_remote)\n",
    "\n",
    "print(\"=== Query vs 'remote work' ===\")\n",
    "print(f\"  dot product          : {dot_product:.4f}\")\n",
    "print(f\"  ||query||            : {norm_query:.4f}\")\n",
    "print(f\"  ||remote work||      : {norm_remote:.4f}\")\n",
    "print(f\"  cosine similarity    : {cosine_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# ---- Compare all three candidates at once using the shared helper ----\n",
    "from rag_tutorials.embeddings import cosine_similarity\n",
    "\n",
    "candidates = np.stack([vec_remote_work, vec_leave_policy, vec_international_transfer])\n",
    "labels     = [\"remote work policy\", \"leave policy\", \"international transfer\"]\n",
    "scores     = cosine_similarity(query_vec, candidates)\n",
    "\n",
    "print(\"Cosine scores for query 'working from abroad':\")\n",
    "for label, score in sorted(zip(labels, scores), key=lambda x: -x[1]):\n",
    "    bar = \"\u2588\" * int(score * 20)\n",
    "    print(f\"  {label:<22} {score:.4f}  {bar}\")\n",
    "print()\n",
    "print(\"Highest score \u2192 retrieved first.  Lowest score \u2192 may not make top-k.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what_is_vector_store",
   "metadata": {},
   "source": [
    "### What Is a Vector Store (Database) and Why Do We Need One?\n",
    "\n",
    "#### The Naive Approach: Compare the Query Against Every Chunk\n",
    "\n",
    "After embedding all chunks, we have a table like this:\n",
    "\n",
    "```\n",
    "chunk_id  text                              vector (1536 numbers)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "  0       \"Remote work: up to 90 days...\"   [0.80, 0.20, 0.50, ...]\n",
    "  1       \"Annual leave entitlement...\"     [0.10, 0.90, 0.30, ...]\n",
    "  2       \"International transfer rules\"    [0.75, 0.25, 0.55, ...]\n",
    "  ...     ...                               ...\n",
    "```\n",
    "\n",
    "To find the most relevant chunks for a query, we *could* compute cosine similarity\n",
    "between the query vector and every single row.  This is called **brute-force search**.\n",
    "\n",
    "```\n",
    "query_vec vs chunk_0  \u2192  0.97  (retrieved)\n",
    "query_vec vs chunk_1  \u2192  0.41\n",
    "query_vec vs chunk_2  \u2192  0.89  (retrieved)\n",
    "...repeat for all N chunks...\n",
    "\u2192 sort \u2192 return top-k\n",
    "```\n",
    "\n",
    "This works fine at small scale.  At large scale it is unacceptably slow:\n",
    "\n",
    "| Number of chunks | Brute-force time (approx.) |\n",
    "|-----------------|---------------------------|\n",
    "| 100 | < 1 ms |\n",
    "| 10,000 | ~10 ms |\n",
    "| 1,000,000 | ~1 second |\n",
    "| 100,000,000 | ~100 seconds (unacceptably slow) |\n",
    "\n",
    "#### The Solution: An Index (ANN \u2014 Approximate Nearest-Neighbor)\n",
    "\n",
    "A **vector store** (like **Chroma**, used in this tutorial) builds an *index* that\n",
    "pre-organises the vectors so most comparisons can be skipped.\n",
    "\n",
    "Chroma uses **HNSW** (Hierarchical Navigable Small World) \u2014 think of it as a map\n",
    "with highways and local roads:\n",
    "\n",
    "```\n",
    "HNSW is a graph where:\n",
    "  \u2022 Each chunk vector is a node\n",
    "  \u2022 Nearby vectors are connected by edges\n",
    "  \u2022 At query time, the search \"hops\" along edges toward the query vector\n",
    "    instead of visiting every node\n",
    "\n",
    "Result: finds the nearest neighbors in O(log N) time instead of O(N)\n",
    "```\n",
    "\n",
    "#### What Chroma Stores for Each Chunk\n",
    "\n",
    "| Stored item | Purpose |\n",
    "|-------------|--------|\n",
    "| Embedding vector | Used for similarity search |\n",
    "| Raw text | Returned in results so the LLM can read it |\n",
    "| Metadata (`doc_id`, `section`) | Allows filtering (e.g. \"only search section X\") |\n",
    "\n",
    "**At query time:**\n",
    "1. The query is embedded \u2192 `query_vec`\n",
    "2. Chroma runs ANN search \u2192 finds `top_k` chunk vectors nearest to `query_vec`\n",
    "3. Returns the corresponding chunk texts + similarity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de9e52",
   "metadata": {},
   "source": [
    "### How Nearest-Neighbor Search Returns Top-k Results\n",
    "\n",
    "**The core problem:** you have a query vector and N chunk vectors in the store.\n",
    "You want the k chunks whose vectors are *closest* to the query \u2014 the **k nearest neighbors**.\n",
    "\n",
    "#### Step-by-step: what happens at query time\n",
    "\n",
    "```\n",
    "1. Embed the query           \u2192 query_vec  (1536 numbers)\n",
    "2. For each chunk vector in the store\n",
    "       score[i] = cosine_similarity(query_vec, chunk_vec[i])\n",
    "3. Sort all scores descending\n",
    "4. Return the top-k chunk texts (highest scores first)\n",
    "```\n",
    "\n",
    "The diagram below uses a tiny 4-chunk example to make every step concrete.\n",
    "\n",
    "```\n",
    "Query: 'working from abroad'\n",
    "\n",
    "chunk_A  'remote work policy'          score: 0.97  \u25c0\u2500\u2500 rank 1  in top-3\n",
    "chunk_B  'annual leave entitlement'    score: 0.41  \u25c0\u2500\u2500 rank 4  (not retrieved)\n",
    "chunk_C  'international transfer rules'score: 0.89  \u25c0\u2500\u2500 rank 2  in top-3\n",
    "chunk_D  'parental leave procedures'   score: 0.55  \u25c0\u2500\u2500 rank 3  in top-3\n",
    "\n",
    "top_k = 3  \u2192  returned: [chunk_A, chunk_C, chunk_D]\n",
    "```\n",
    "\n",
    "#### Exact vs Approximate Nearest-Neighbor (ANN)\n",
    "\n",
    "| Approach | How it works | When used |\n",
    "|----------|--------------|-----------|\n",
    "| **Exact (brute-force)** | Compare query against every vector | Small datasets |\n",
    "| **Approximate (ANN)** | Build an index (e.g., HNSW graph) that skips most comparisons | Large datasets |\n",
    "\n",
    "Chroma uses **HNSW** (Hierarchical Navigable Small World) by default \u2014 it builds a\n",
    "graph of vectors where nearby vectors are connected.  At query time it traverses\n",
    "the graph greedily, visiting only a small fraction of all vectors, yet finds the\n",
    "nearest neighbors with high probability.\n",
    "\n",
    "> **Key insight:** top-k is not a threshold \u2014 it is a *count*.  No matter how\n",
    "> dissimilar the best chunk is, the system always returns exactly k results.  A\n",
    "> high cosine score means \"very relevant\"; a low score means \"best we could find\n",
    "> but probably not very relevant\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e81962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest-neighbor top-k walkthrough \u2014 toy example\n",
    "# Shows exactly how the vector store picks which chunks to return.\n",
    "\n",
    "import numpy as np\n",
    "from rag_tutorials.embeddings import cosine_similarity\n",
    "\n",
    "# \u2500\u2500 6 toy chunk vectors (3-dim for readability; real ones are 1536-dim) \u2500\u2500\n",
    "chunk_vectors = np.array([\n",
    "    [0.80, 0.20, 0.50],   # chunk 0: 'remote work policy'\n",
    "    [0.10, 0.90, 0.30],   # chunk 1: 'annual leave entitlement'\n",
    "    [0.75, 0.25, 0.55],   # chunk 2: 'international transfer rules'\n",
    "    [0.15, 0.70, 0.40],   # chunk 3: 'parental leave procedures'\n",
    "    [0.60, 0.35, 0.65],   # chunk 4: 'home-office equipment policy'\n",
    "    [0.05, 0.95, 0.20],   # chunk 5: 'sick leave documentation'\n",
    "])\n",
    "chunk_labels = [\n",
    "    \"remote work policy\",\n",
    "    \"annual leave entitlement\",\n",
    "    \"international transfer rules\",\n",
    "    \"parental leave procedures\",\n",
    "    \"home-office equipment policy\",\n",
    "    \"sick leave documentation\",\n",
    "]\n",
    "\n",
    "query_vec = np.array([0.85, 0.15, 0.45])   # query: 'working from abroad'\n",
    "TOP_K = 3\n",
    "\n",
    "# \u2500\u2500 Step 1: compute cosine similarity to every chunk \u2500\u2500\n",
    "scores = cosine_similarity(query_vec, chunk_vectors)\n",
    "\n",
    "# \u2500\u2500 Step 2: rank by descending score \u2500\u2500\n",
    "ranked_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "print(f\"Query: 'working from abroad'\")\n",
    "print(f\"\\nAll {len(chunk_labels)} chunks ranked by cosine similarity:\")\n",
    "print(f\"{'Rank':<5} {'Score':>6}  {'Chunk label'}\")\n",
    "print(\"-\" * 50)\n",
    "for rank, idx in enumerate(ranked_indices, 1):\n",
    "    selected = \" \u25c0 top-k\" if rank <= TOP_K else \"\"\n",
    "    bar = \"\u2588\" * int(scores[idx] * 20)\n",
    "    print(f\"  {rank:<4} {scores[idx]:.4f}  {chunk_labels[idx]:<30} {bar}{selected}\")\n",
    "\n",
    "# \u2500\u2500 Step 3: return top-k \u2500\u2500\n",
    "top_k_indices = ranked_indices[:TOP_K]\n",
    "print(f\"\\n\u2192 top_k={TOP_K} chunks returned to the LLM:\")\n",
    "for i, idx in enumerate(top_k_indices, 1):\n",
    "    print(f\"  {i}. [{scores[idx]:.4f}] {chunk_labels[idx]}\")\n",
    "\n",
    "print(\"\\n\u2192 chunks NOT retrieved (score too low for top-k):\")\n",
    "for idx in ranked_indices[TOP_K:]:\n",
    "    print(f\"  [ ] [{scores[idx]:.4f}] {chunk_labels[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e644a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (24, 1536)\n",
      "Example vector (first 10 dims): [ 0.0294  0.0544  0.0442  0.0226  0.002  -0.0399  0.0043  0.0554  0.0151\n",
      "  0.0031]\n",
      "Toy chunk 1 cosine score: 0.4026\n",
      "Toy chunk 2 cosine score: 0.2809\n",
      "Toy chunk 3 cosine score: 0.3770\n"
     ]
    }
   ],
   "source": [
    "# 5) Create Embeddings and Build Vector Index\n",
    "# Each chunk text is converted into a high-dimensional vector (1536 dims for text-embedding-3-small).\n",
    "# These vectors are stored in Chroma so we can search by cosine similarity at query time.\n",
    "\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.embeddings import embed_texts, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "dense_retriever, doc_vectors = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial1_basic_dense\",\n",
    "    embedding_model=cfg.embedding_model,\n",
    ")\n",
    "\n",
    "# doc_vectors shape: (num_chunks, embedding_dim)\n",
    "# Each row is one chunk's vector; columns are learned numeric features.\n",
    "print(\"Embedding matrix shape (chunks \u00d7 dims):\", doc_vectors.shape)\n",
    "print(\"First chunk vector \u2014 first 10 of\", doc_vectors.shape[1], \"dimensions:\")\n",
    "print(\" \", np.round(doc_vectors[0][:10], 4))\n",
    "print(\"  (every dimension encodes a subtle aspect of meaning)\")\n",
    "print()\n",
    "\n",
    "# --- Real-embedding cosine similarity trace using 3 actual chunks ---\n",
    "# We embed the same query and three chunks with the real model so you can\n",
    "# see that the pattern from the toy demo above holds for real vectors too.\n",
    "sample_texts = [chunks[i].text for i in range(3)]\n",
    "sample_vectors = embed_texts(sample_texts, model=cfg.embedding_model)\n",
    "sample_query = \"What is the policy for working from another country?\"\n",
    "sample_query_vector = embed_texts([sample_query], model=cfg.embedding_model)[0]\n",
    "\n",
    "scores = cosine_similarity(sample_query_vector, sample_vectors)\n",
    "print(\"Real cosine similarity scores (query vs first 3 chunks):\")\n",
    "for idx, (score, text) in enumerate(zip(scores, sample_texts), start=1):\n",
    "    bar = \"\u2588\" * int(score * 20)\n",
    "    print(f\"  Chunk {idx} score={score:.4f}  {bar}\")\n",
    "    print(f\"    preview: {text[:80]}...\")\n",
    "print()\n",
    "print(\"The full index contains\", doc_vectors.shape[0], \"chunks; Chroma runs the same\")\n",
    "print(\"cosine comparison for ALL of them and returns the top-k highest scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67817acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>score</th>\n",
       "      <th>source</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-FIX-00</td>\n",
       "      <td>0.168518</td>\n",
       "      <td>dense</td>\n",
       "      <td>Working from another country is capped at 14 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-FIX-02</td>\n",
       "      <td>0.084514</td>\n",
       "      <td>dense</td>\n",
       "      <td>ome countries require pre-travel right-to-work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DOC-HB-INTERNATIONALTAX-FIX-00</td>\n",
       "      <td>-0.031337</td>\n",
       "      <td>dense</td>\n",
       "      <td>Employees traveling internationally may need F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-FIX-01</td>\n",
       "      <td>-0.095470</td>\n",
       "      <td>dense</td>\n",
       "      <td>xposure. Employees must submit destination cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>DOC-HB-REMOTEWORK-FIX-03</td>\n",
       "      <td>-0.113855</td>\n",
       "      <td>dense</td>\n",
       "      <td>view ergonomic setup guidance quarterly and co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                         chunk_id     score source  \\\n",
       "0     1  DOC-HB-INTERNATIONALWORK-FIX-00  0.168518  dense   \n",
       "1     2  DOC-HB-INTERNATIONALWORK-FIX-02  0.084514  dense   \n",
       "2     3   DOC-HB-INTERNATIONALTAX-FIX-00 -0.031337  dense   \n",
       "3     4  DOC-HB-INTERNATIONALWORK-FIX-01 -0.095470  dense   \n",
       "4     5         DOC-HB-REMOTEWORK-FIX-03 -0.113855  dense   \n",
       "\n",
       "                                             preview  \n",
       "0  Working from another country is capped at 14 d...  \n",
       "1  ome countries require pre-travel right-to-work...  \n",
       "2  Employees traveling internationally may need F...  \n",
       "3  xposure. Employees must submit destination cou...  \n",
       "4  view ergonomic setup guidance quarterly and co...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Implement Retriever Logic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def retrieve_dense(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "probe_query = \"What is the policy for working from another country?\"\n",
    "probe_results = retrieve_dense(probe_query, top_k=cfg.top_k)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\n",
    "        \"rank\": idx + 1,\n",
    "        \"chunk_id\": row.chunk_id,\n",
    "        \"score\": row.score,\n",
    "        \"source\": row.source,\n",
    "        \"preview\": row.text,\n",
    "    }\n",
    "    for idx, row in enumerate(probe_results)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "334d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe question: What is the policy for working from another country?\n",
      "Retrieved context: ['Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, employees must open a Global Mobility case and obtain HR, Legal, and Payroll approval. Violations can trigger immigration, payroll, and tax e', 'ome countries require pre-travel right-to-work checks even for short stays under the 14-day cap. International work days are counted using local calendar dates at destination, not departure timezone timestamps. Repeated short trips to the same country can accu', 'Employees traveling internationally may need Form A-12 before departure when cross-border work exceeds 7 business days. The tax team uses Form A-12 to assess treaty relief, withholding obligations, and permanent establishment risk. Form A-12 submissions should', 'xposure. Employees must submit destination country, travel dates, host entity, and work purpose when opening the Global Mobility case. Approval decisions depend on role type, customer access level, and whether on-site activities include contract negotiation. S', 'view ergonomic setup guidance quarterly and complete the annual safety attestation in the HR portal. Temporary domestic work from a location outside the home office state may require payroll location review if extended beyond 30 days. Use of personal devices f']\n",
      "Employees can work from another country up to 14 days in a rolling 12-month period without permit support. Beyond 14 days, they must open a Global Mobility case and obtain HR, Legal, and Payroll approval. Additional requirements include submitting details like destination country, travel dates, host entity, and work purpose for approval. Violations can lead to immigration, payroll, and tax exposure [Chunk 1, Chunk 4].\n"
     ]
    }
   ],
   "source": [
    "# 7) Implement Prompt Template and LLM Call\n",
    "# RAG injects the retrieved chunks directly into the LLM prompt.\n",
    "# We print the full prompt below so you can see exactly what the model receives.\n",
    "\n",
    "from rag_tutorials.qa import answer_with_context, build_context\n",
    "\n",
    "def rag_answer(question: str, top_k: int = 5):\n",
    "    retrieved = retrieve_dense(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=cfg.chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "# --- Show the actual prompt that is sent to the LLM ---\n",
    "context_chunks = [r.text for r in probe_results]\n",
    "context_block = build_context(context_chunks)\n",
    "full_prompt = (\n",
    "    \"You are a policy assistant. Answer only from the provided context. \"\n",
    "    \"If the answer is not present, say you do not have enough context.\\n\\n\"\n",
    "    f\"Question: {probe_query}\\n\\n\"\n",
    "    f\"Context:\\n{context_block}\\n\\n\"\n",
    "    \"Provide a concise answer and include a short citation like [Chunk 1].\"\n",
    ")\n",
    "print(\"=\" * 60)\n",
    "print(\"FULL PROMPT SENT TO LLM:\")\n",
    "print(\"=\" * 60)\n",
    "print(full_prompt)\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"The LLM only sees the text above \u2014 it cannot look outside this prompt.\")\n",
    "print(\"This is what 'grounding' means: the answer must come from these chunks.\")\n",
    "print()\n",
    "\n",
    "print(\"Probe question:\", probe_query)\n",
    "answer, retrieved = rag_answer(probe_query)\n",
    "print(\"LLM answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_explainer",
   "metadata": {},
   "source": [
    "### How to Read the Evaluation Metrics\n",
    "\n",
    "After running the RAG pipeline on a set of test queries, we measure how well it\n",
    "performed.  Four metrics are used throughout all five tutorials.\n",
    "\n",
    "#### Recall@k \u2014 Did we retrieve the right chunk?\n",
    "\n",
    "**Plain English:** Out of all test queries, what fraction of the time did the\n",
    "correct source document appear *anywhere* in the top-k retrieved chunks?\n",
    "\n",
    "```\n",
    "Recall@5 = (queries where correct source was in top-5) / (total queries)\n",
    "\n",
    "Example: 8 out of 10 queries retrieved the correct chunk \u2192 Recall@5 = 0.80\n",
    "```\n",
    "\n",
    "| Value | Meaning |\n",
    "|:-----:|---------|\n",
    "| **1.0** | Perfect \u2014 the right chunk was always in the top-k |\n",
    "| **0.8** | 80 % of queries succeeded \u2014 20 % missed the right chunk |\n",
    "| **0.5** | Only half the queries retrieved the right chunk |\n",
    "| **0.0** | The retriever never found the right source |\n",
    "\n",
    "> Note: Recall@k does **not** care about *where* in the top-k the correct chunk appears \u2014\n",
    "> just whether it's present at all.  MRR (below) measures the position.\n",
    "\n",
    "#### MRR \u2014 Mean Reciprocal Rank \u2014 Was the right chunk near the top?\n",
    "\n",
    "**Plain English:** When the correct chunk *is* retrieved, how highly ranked is it?\n",
    "Being ranked 1st is much more useful than being ranked 5th.\n",
    "\n",
    "```\n",
    "For each query: reciprocal_rank = 1 / (position of first correct chunk)\n",
    "\n",
    "  Correct chunk at rank 1 \u2192 1/1 = 1.00\n",
    "  Correct chunk at rank 2 \u2192 1/2 = 0.50\n",
    "  Correct chunk at rank 3 \u2192 1/3 = 0.33\n",
    "  Not found at all        \u2192 0\n",
    "\n",
    "MRR = average of all reciprocal ranks across queries\n",
    "```\n",
    "\n",
    "| MRR Value | Meaning |\n",
    "|:---------:|---------|\n",
    "| **1.0** | Always ranked 1st |\n",
    "| **0.5** | On average, correct chunk is at rank 2 |\n",
    "| **0.33** | On average, correct chunk is at rank 3 |\n",
    "\n",
    "#### Groundedness \u2014 Did the answer come from the retrieved context?\n",
    "\n",
    "**Plain English:** What fraction of the LLM's answer words appear in the retrieved\n",
    "chunks?  A high score means the model is *using* the retrieved context, not inventing.\n",
    "\n",
    "```\n",
    "Answer:  \"Employees may work remotely for up to 90 days per year.\"\n",
    "Context: \"...may work remotely for up to 90 days per year with manager approval...\"\n",
    "\n",
    "Most answer words appear in context \u2192 groundedness \u2248 0.85  (good!)\n",
    "```\n",
    "\n",
    "| Value | Meaning |\n",
    "|:-----:|---------|\n",
    "| **0.8\u20131.0** | Highly grounded \u2014 answer closely follows retrieved text |\n",
    "| **0.5\u20130.79** | Partially grounded \u2014 some invention |\n",
    "| **< 0.5** | Likely hallucinating \u2014 answer not supported by context |\n",
    "\n",
    "#### Latency (ms) \u2014 How fast is the pipeline?\n",
    "\n",
    "**Plain English:** Total wall-clock time in milliseconds from question to answer,\n",
    "including embedding, retrieval, and LLM generation.\n",
    "\n",
    "```\n",
    "  ~100\u2013300 ms  \u2192  retrieval only (no LLM)\n",
    " ~500\u20132000 ms  \u2192  full RAG pipeline\n",
    ">5000 ms       \u2192  unusable for real-time chat\n",
    "```\n",
    "\n",
    "| Metric | Range | Higher = better? |\n",
    "|--------|-------|------------------|\n",
    "| **Recall@k** | 0 \u2013 1 | Yes |\n",
    "| **MRR** | 0 \u2013 1 | Yes |\n",
    "| **Groundedness** | 0 \u2013 1 | Yes |\n",
    "| **Latency (ms)** | > 0 | No (lower is better) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49376064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutorial 1 metrics: {'recall_at_k': 1.0, 'mrr': 0.975, 'latency_ms': 1826.7711563268676, 'groundedness': 0.7853514886656859}\n",
      "Retrieved context: ['Z-Tech encourages remote work from home, co-working spaces, or temporary domestic locations. Employees must stay reachable during assigned timezone hours and use approved managed devices. Public Wi-Fi usage is allowed only with corporate VPN enabled. Employees', 'view ergonomic setup guidance quarterly and complete the annual safety attestation in the HR portal. Temporary domestic work from a location outside the home office state may require payroll location review if extended beyond 30 days. Use of personal devices f', 'ffs. Managers may define team-specific overlap hours when projects involve coordination across offices in different time zones. Home-office expenses are reimbursable only for pre-approved categories listed in the internal procurement guide. Employees should re', 'Working from another country is capped at 14 days in a rolling 12-month period without permit support. Beyond 14 days, employees must open a Global Mobility case and obtain HR, Legal, and Payroll approval. Violations can trigger immigration, payroll, and tax e', 'Employees handling customer data while traveling must use VPN, hardware-backed MFA, and encrypted storage. Lost or stolen devices must be reported within one hour to security operations. Use of public charging stations is discouraged unless a data-blocking ada']\n",
      "\n",
      "Query: Can I work remotely from cafes and home?\n",
      "1. DOC-HB-REMOTEWORK-FIX-00 | score=0.1312 | Z-Tech encourages remote work from home, co-working spaces, or temporary domestic locations. Employe\n",
      "2. DOC-HB-REMOTEWORK-FIX-03 | score=-0.1121 | view ergonomic setup guidance quarterly and complete the annual safety attestation in the HR portal.\n",
      "3. DOC-HB-REMOTEWORK-FIX-02 | score=-0.1139 | ffs. Managers may define team-specific overlap hours when projects involve coordination across offic\n",
      "4. DOC-HB-INTERNATIONALWORK-FIX-00 | score=-0.2463 | Working from another country is capped at 14 days in a rolling 12-month period without permit suppor\n",
      "5. DOC-HB-SECURITY-FIX-00 | score=-0.2610 | Employees handling customer data while traveling must use VPN, hardware-backed MFA, and encrypted st\n",
      "\n",
      "Answer: Yes, you can work remotely from home and cafes or co-working spaces, but you must stay reachable during assigned hours, use approved devices, and enable corporate VPN on public Wi-Fi. Temporary domestic remote work outside your home office state is allowed but may require payroll review if over 30 days [Chunk 1].\n"
     ]
    }
   ],
   "source": [
    "# 8) Assemble End-to-End RAG Pipeline + 9/10 Smoke Tests and Evaluation\n",
    "\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "sample_queries = queries[: cfg.sample_eval_size]\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: retrieve_dense(question, top_k=cfg.top_k),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=cfg.chat_model),\n",
    "        top_k=cfg.top_k,\n",
    "    )\n",
    "    for q in sample_queries\n",
    "]\n",
    "\n",
    "metrics = summarize(rows)\n",
    "print(\"Tutorial 1 metrics:\", metrics)\n",
    "\n",
    "# Show one trace row for novice debugging\n",
    "trace = sample_queries[0]\n",
    "trace_answer, trace_retrieved = rag_answer(trace.question, top_k=cfg.top_k)\n",
    "print(\"\\nQuery:\", trace.question)\n",
    "for idx, row in enumerate(trace_retrieved, start=1):\n",
    "    print(f\"{idx}. {row.chunk_id} | score={row.score:.4f} | {row.text[:100]}\")\n",
    "print(\"\\nAnswer:\", trace_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}