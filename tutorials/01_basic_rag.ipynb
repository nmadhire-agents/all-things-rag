{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622af14c",
   "metadata": {},
   "source": [
    "# Tutorial 1 — Basic RAG (Dense Retrieval Baseline)\n",
    "\n",
    "This notebook implements a complete baseline RAG pipeline and makes embeddings + retrieval transparent for first-time learners.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Documents] --> B[Fixed Chunking]\n",
    "    B --> C[OpenAI Embeddings]\n",
    "    C --> D[Chroma Vector Index]\n",
    "    E[User Query] --> F[Query Embedding]\n",
    "    F --> D\n",
    "    D --> G[Top-k Chunks]\n",
    "    G --> H[LLM Answer]\n",
    "```\n",
    "\n",
    "Continuity note:\n",
    "- Tutorial 2 keeps the same pipeline but changes **chunking**.\n",
    "- Tutorial 3 keeps chunking and adds **reranking**.\n",
    "- Tutorial 4 adds **hybrid retrieval** (keyword + dense)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed2d17",
   "metadata": {},
   "source": [
    "## Learning checkpoint: what works vs what breaks\n",
    "\n",
    "**What works in Tutorial 1**\n",
    "- Dense retrieval can find generally related handbook content.\n",
    "- End-to-end RAG flow is functional (ingest → chunk → embed → retrieve → answer).\n",
    "\n",
    "**Challenges you should observe**\n",
    "- Query intent can be too broad for nearest-neighbor retrieval.\n",
    "- Exception-heavy policy questions may return partially relevant chunks.\n",
    "- Exact policy identifiers (like forms/codes) are often weakly handled.\n",
    "\n",
    "**Why move to Tutorial 2**\n",
    "- The first bottleneck is chunk quality.\n",
    "- We next improve *how text is split* so policy context stays intact before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Set Up Environment and Dependencies\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "]\n",
    "\n",
    "missing = [pkg for pkg in REQUIRED_PACKAGES if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Run: uv sync\")\n",
    "else:\n",
    "    print(\"All required packages are available.\")\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Working directory:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c12bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define Configuration and Paths\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    embedding_model: str = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    chat_model: str = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "    chunk_mode: str = \"fixed\"\n",
    "    top_k: int = 5\n",
    "    sample_eval_size: int = 20\n",
    "    handbook_path: str = \"data/handbook_manual.txt\"\n",
    "    queries_path: str = \"data/queries.jsonl\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Copy .env.example to .env and set your key.\")\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load and Normalize Source Documents (shared handbook text + query set)\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "\n",
    "if not Path(cfg.handbook_path).exists() or not Path(cfg.queries_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Shared data files are missing. Run: uv run python scripts/generate_data.py\"\n",
    "    )\n",
    "\n",
    "documents = load_handbook_documents(cfg.handbook_path)\n",
    "queries = load_queries(cfg.queries_path)\n",
    "\n",
    "print(\"Source text:\", cfg.handbook_path)\n",
    "print(\"Parsed handbook sections:\", len(documents))\n",
    "print(\"Queries:\", len(queries))\n",
    "print(\"Sample parsed document:\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac16d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Split Documents into Chunks (fixed chunking baseline)\n",
    "\n",
    "from dataclasses import asdict\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "import pandas as pd\n",
    "\n",
    "chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "\n",
    "chunk_df = pd.DataFrame([asdict(c) for c in chunks])\n",
    "stats = {\n",
    "    \"chunk_count\": len(chunk_df),\n",
    "    \"avg_chunk_chars\": chunk_df.text.map(len).mean(),\n",
    "    \"max_chunk_chars\": chunk_df.text.map(len).max(),\n",
    "}\n",
    "print(stats)\n",
    "chunk_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14881ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9ec69",
   "metadata": {},
   "source": [
    "## Novice Lens: How Embeddings and Retrieval Actually Work\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User Query\n",
    "    participant E as Embedding Model\n",
    "    participant V as Vector Store\n",
    "    participant L as LLM\n",
    "    U->>E: \"working from another country\"\n",
    "    E->>V: query vector\n",
    "    V-->>U: top-k chunks + scores\n",
    "    U->>L: question + retrieved chunks\n",
    "    L-->>U: grounded answer\n",
    "```\n",
    "\n",
    "We will inspect:\n",
    "1. Query and chunk vectors (dimensions and first values)\n",
    "2. Cosine similarities\n",
    "3. Ranked chunk IDs returned to the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e644a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Create Embeddings and Build Vector Index\n",
    "\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.embeddings import embed_texts, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "dense_retriever, doc_vectors = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial1_basic_dense\",\n",
    "    embedding_model=cfg.embedding_model,\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix shape:\", doc_vectors.shape)\n",
    "print(\"Example vector (first 10 dims):\", np.round(doc_vectors[0][:10], 4))\n",
    "\n",
    "# Tiny toy example for intuition\n",
    "sample_chunks = [chunks[i].text for i in range(3)]\n",
    "sample_vectors = embed_texts(sample_chunks, model=cfg.embedding_model)\n",
    "sample_query = \"What is the policy for working from another country?\"\n",
    "sample_query_vector = embed_texts([sample_query], model=cfg.embedding_model)[0]\n",
    "\n",
    "scores = cosine_similarity(sample_query_vector, sample_vectors)\n",
    "for idx, score in enumerate(scores, start=1):\n",
    "    print(f\"Toy chunk {idx} cosine score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67817acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Implement Retriever Logic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def retrieve_dense(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "probe_query = \"What is the policy for working from another country?\"\n",
    "probe_results = retrieve_dense(probe_query, top_k=cfg.top_k)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\n",
    "        \"rank\": idx + 1,\n",
    "        \"chunk_id\": row.chunk_id,\n",
    "        \"score\": row.score,\n",
    "        \"source\": row.source,\n",
    "        \"preview\": row.text[:120],\n",
    "    }\n",
    "    for idx, row in enumerate(probe_results)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Implement Prompt Template and LLM Call\n",
    "\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "\n",
    "def rag_answer(question: str, top_k: int = 5):\n",
    "    retrieved = retrieve_dense(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=cfg.chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "answer, retrieved = rag_answer(probe_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49376064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Assemble End-to-End RAG Pipeline + 9/10 Smoke Tests and Evaluation\n",
    "\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "sample_queries = queries[: cfg.sample_eval_size]\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: retrieve_dense(question, top_k=cfg.top_k),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=cfg.chat_model),\n",
    "        top_k=cfg.top_k,\n",
    "    )\n",
    "    for q in sample_queries\n",
    "]\n",
    "\n",
    "metrics = summarize(rows)\n",
    "print(\"Tutorial 1 metrics:\", metrics)\n",
    "\n",
    "# Show one trace row for novice debugging\n",
    "trace = sample_queries[0]\n",
    "trace_answer, trace_retrieved = rag_answer(trace.question, top_k=cfg.top_k)\n",
    "print(\"\\nQuery:\", trace.question)\n",
    "for idx, row in enumerate(trace_retrieved, start=1):\n",
    "    print(f\"{idx}. {row.chunk_id} | score={row.score:.4f} | {row.text[:100]}\")\n",
    "print(\"\\nAnswer:\", trace_answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
