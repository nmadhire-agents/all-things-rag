{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622af14c",
   "metadata": {},
   "source": [
    "# Tutorial 1 — Basic RAG (Dense Retrieval Baseline)\n",
    "\n",
    "This notebook implements a complete baseline RAG pipeline and makes embeddings + retrieval transparent for first-time learners.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Documents] --> B[Fixed Chunking]\n",
    "    B --> C[OpenAI Embeddings]\n",
    "    C --> D[Chroma Vector Index]\n",
    "    E[User Query] --> F[Query Embedding]\n",
    "    F --> D\n",
    "    D --> G[Top-k Chunks]\n",
    "    G --> H[LLM Answer]\n",
    "```\n",
    "\n",
    "Continuity note:\n",
    "- Tutorial 2 keeps the same pipeline but changes **chunking**.\n",
    "- Tutorial 3 keeps chunking and adds **reranking**.\n",
    "- Tutorial 4 adds **hybrid retrieval** (keyword + dense)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed2d17",
   "metadata": {},
   "source": [
    "## Learning checkpoint: what works vs what breaks\n",
    "\n",
    "**What works in Tutorial 1**\n",
    "- Dense retrieval can find generally related handbook content.\n",
    "- End-to-end RAG flow is functional (ingest → chunk → embed → retrieve → answer).\n",
    "\n",
    "**Challenges you should observe**\n",
    "- Query intent can be too broad for nearest-neighbor retrieval.\n",
    "- Exception-heavy policy questions may return partially relevant chunks.\n",
    "- Exact policy identifiers (like forms/codes) are often weakly handled.\n",
    "\n",
    "**Why move to Tutorial 2**\n",
    "- The first bottleneck is chunk quality.\n",
    "- We next improve *how text is split* so policy context stays intact before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003b42c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing packages: ['rank_bm25']\n",
      "Running: uv sync\n",
      "Python: 3.11.13\n",
      "Working directory: /Users/avy/GitHubProjects/allagents/all-things-rag\n",
      "Repo root: /Users/avy/GitHubProjects/allagents/all-things-rag\n",
      "Using src path: /Users/avy/GitHubProjects/allagents/all-things-rag/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m204 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m180 packages\u001b[0m \u001b[2min 35ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1) Set Up Environment and Dependencies\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure uv is available (installs with: pip install uv)\n",
    "if shutil.which(\"uv\") is None:\n",
    "    print(\"uv not found. Installing with pip...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"], check=True)\n",
    "\n",
    "# Ensure notebook runs from repo root and local src/ is importable\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next(\n",
    "    (path for path in [cwd, *cwd.parents] if (path / \"pyproject.toml\").exists() and (path / \"src\").exists()),\n",
    "    cwd,\n",
    ")\n",
    "os.chdir(repo_root)\n",
    "src_path = repo_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"openai\",\n",
    "    \"chromadb\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"rank_bm25\",\n",
    "    \"sentence_transformers\",\n",
    "    \"dotenv\",\n",
    "]\n",
    "\n",
    "missing = [pkg for pkg in REQUIRED_PACKAGES if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Running: uv sync\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "else:\n",
    "    print(\"All required packages are available.\")\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"Using src path:\", src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c12bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(embedding_model='text-embedding-3-small', chat_model='gpt-4.1-mini', chunk_mode='fixed', top_k=5, sample_eval_size=20, handbook_path='data/handbook_manual.txt', queries_path='data/queries.jsonl')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Define Configuration and Paths\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    embedding_model: str = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    chat_model: str = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "    chunk_mode: str = \"fixed\"\n",
    "    top_k: int = 5\n",
    "    sample_eval_size: int = 20\n",
    "    handbook_path: str = \"data/handbook_manual.txt\"\n",
    "    queries_path: str = \"data/queries.jsonl\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Copy .env.example to .env and set your key.\")\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d81fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text: data/handbook_manual.txt\n",
      "Parsed handbook sections: 5\n",
      "Queries: 200\n",
      "Sample parsed document: Document(doc_id='DOC-HB-REMOTEWORK', title='Z-Tech Handbook - Remote Work', section='Remote Work', text='Z-Tech encourages remote work from home, co-working spaces, or temporary domestic locations. Employees must stay reachable during assigned timezone hours and use approved managed devices. Public Wi-Fi usage is allowed only with corporate VPN enabled.')\n"
     ]
    }
   ],
   "source": [
    "# 3) Load and Normalize Source Documents (shared handbook text + query set)\n",
    "\n",
    "from rag_tutorials.io_utils import load_handbook_documents, load_queries\n",
    "\n",
    "if not Path(cfg.handbook_path).exists() or not Path(cfg.queries_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Shared data files are missing. Run: uv run python scripts/generate_data.py\"\n",
    "    )\n",
    "\n",
    "documents = load_handbook_documents(cfg.handbook_path)\n",
    "queries = load_queries(cfg.queries_path)\n",
    "\n",
    "print(\"Source text:\", cfg.handbook_path)\n",
    "print(\"Parsed handbook sections:\", len(documents))\n",
    "print(\"Queries:\", len(queries))\n",
    "print(\"Sample parsed document:\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac16d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_count': 6, 'avg_chunk_chars': np.float64(191.0), 'max_chunk_chars': np.int64(260)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOC-HB-REMOTEWORK-FIX-00</td>\n",
       "      <td>DOC-HB-REMOTEWORK</td>\n",
       "      <td>Remote Work</td>\n",
       "      <td>Z-Tech encourages remote work from home, co-wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-FIX-00</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK</td>\n",
       "      <td>International Work</td>\n",
       "      <td>Working from another country is capped at 14 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOC-HB-INTERNATIONALWORK-FIX-01</td>\n",
       "      <td>DOC-HB-INTERNATIONALWORK</td>\n",
       "      <td>International Work</td>\n",
       "      <td>xposure.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          chunk_id                    doc_id  \\\n",
       "0         DOC-HB-REMOTEWORK-FIX-00         DOC-HB-REMOTEWORK   \n",
       "1  DOC-HB-INTERNATIONALWORK-FIX-00  DOC-HB-INTERNATIONALWORK   \n",
       "2  DOC-HB-INTERNATIONALWORK-FIX-01  DOC-HB-INTERNATIONALWORK   \n",
       "\n",
       "              section                                               text  \n",
       "0         Remote Work  Z-Tech encourages remote work from home, co-wo...  \n",
       "1  International Work  Working from another country is capped at 14 d...  \n",
       "2  International Work                                           xposure.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Split Documents into Chunks (fixed chunking baseline)\n",
    "\n",
    "from dataclasses import asdict\n",
    "from rag_tutorials.chunking import fixed_chunk_documents\n",
    "import pandas as pd\n",
    "\n",
    "chunks = fixed_chunk_documents(documents, chunk_size=260)\n",
    "\n",
    "chunk_df = pd.DataFrame([asdict(c) for c in chunks])\n",
    "stats = {\n",
    "    \"chunk_count\": len(chunk_df),\n",
    "    \"avg_chunk_chars\": chunk_df.text.map(len).mean(),\n",
    "    \"max_chunk_chars\": chunk_df.text.map(len).max(),\n",
    "}\n",
    "print(stats)\n",
    "chunk_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14881ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk boundary visualization (same source text, different split strategies)\n",
    "\n",
    "from rag_tutorials.chunking import semantic_chunk_documents\n",
    "\n",
    "section_doc = next(doc for doc in documents if doc.section == \"International Work\")\n",
    "fixed_view = [c.text for c in fixed_chunk_documents([section_doc], chunk_size=120)]\n",
    "semantic_view = [c.text for c in semantic_chunk_documents([section_doc])]\n",
    "\n",
    "print(\"Section:\", section_doc.section)\n",
    "print(\"\\nFixed chunks:\")\n",
    "for idx, chunk_text in enumerate(fixed_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")\n",
    "\n",
    "print(\"\\nSemantic chunks:\")\n",
    "for idx, chunk_text in enumerate(semantic_view, start=1):\n",
    "    print(f\"[{idx}] {chunk_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9ec69",
   "metadata": {},
   "source": [
    "## Novice Lens: How Embeddings and Retrieval Actually Work\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User Query\n",
    "    participant E as Embedding Model\n",
    "    participant V as Vector Store\n",
    "    participant L as LLM\n",
    "    U->>E: \"working from another country\"\n",
    "    E->>V: query vector\n",
    "    V-->>U: top-k chunks + scores\n",
    "    U->>L: question + retrieved chunks\n",
    "    L-->>U: grounded answer\n",
    "```\n",
    "\n",
    "We will inspect:\n",
    "1. Query and chunk vectors (dimensions and first values)\n",
    "2. Cosine similarities\n",
    "3. Ranked chunk IDs returned to the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e644a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Create Embeddings and Build Vector Index\n",
    "\n",
    "from rag_tutorials.pipeline import build_dense_retriever\n",
    "from rag_tutorials.embeddings import embed_texts, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "dense_retriever, doc_vectors = build_dense_retriever(\n",
    "    chunks=chunks,\n",
    "    collection_name=\"tutorial1_basic_dense\",\n",
    "    embedding_model=cfg.embedding_model,\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix shape:\", doc_vectors.shape)\n",
    "print(\"Example vector (first 10 dims):\", np.round(doc_vectors[0][:10], 4))\n",
    "\n",
    "# Tiny toy example for intuition\n",
    "sample_chunks = [chunks[i].text for i in range(3)]\n",
    "sample_vectors = embed_texts(sample_chunks, model=cfg.embedding_model)\n",
    "sample_query = \"What is the policy for working from another country?\"\n",
    "sample_query_vector = embed_texts([sample_query], model=cfg.embedding_model)[0]\n",
    "\n",
    "scores = cosine_similarity(sample_query_vector, sample_vectors)\n",
    "for idx, score in enumerate(scores, start=1):\n",
    "    print(f\"Toy chunk {idx} cosine score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67817acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Implement Retriever Logic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def retrieve_dense(question: str, top_k: int = 5):\n",
    "    return dense_retriever(question, top_k=top_k)\n",
    "\n",
    "probe_query = \"What is the policy for working from another country?\"\n",
    "probe_results = retrieve_dense(probe_query, top_k=cfg.top_k)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\n",
    "        \"rank\": idx + 1,\n",
    "        \"chunk_id\": row.chunk_id,\n",
    "        \"score\": row.score,\n",
    "        \"source\": row.source,\n",
    "        \"preview\": row.text[:120],\n",
    "    }\n",
    "    for idx, row in enumerate(probe_results)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Implement Prompt Template and LLM Call\n",
    "\n",
    "from rag_tutorials.qa import answer_with_context\n",
    "\n",
    "def rag_answer(question: str, top_k: int = 5):\n",
    "    retrieved = retrieve_dense(question, top_k=top_k)\n",
    "    context = [r.text for r in retrieved]\n",
    "    answer = answer_with_context(question, context, model=cfg.chat_model)\n",
    "    return answer, retrieved\n",
    "\n",
    "answer, retrieved = rag_answer(probe_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49376064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Assemble End-to-End RAG Pipeline + 9/10 Smoke Tests and Evaluation\n",
    "\n",
    "from rag_tutorials.evaluation import evaluate_single, summarize\n",
    "\n",
    "sample_queries = queries[: cfg.sample_eval_size]\n",
    "rows = [\n",
    "    evaluate_single(\n",
    "        query=q,\n",
    "        retrieval_fn=lambda question: retrieve_dense(question, top_k=cfg.top_k),\n",
    "        answer_fn=lambda question, context: answer_with_context(question, context, model=cfg.chat_model),\n",
    "        top_k=cfg.top_k,\n",
    "    )\n",
    "    for q in sample_queries\n",
    "]\n",
    "\n",
    "metrics = summarize(rows)\n",
    "print(\"Tutorial 1 metrics:\", metrics)\n",
    "\n",
    "# Show one trace row for novice debugging\n",
    "trace = sample_queries[0]\n",
    "trace_answer, trace_retrieved = rag_answer(trace.question, top_k=cfg.top_k)\n",
    "print(\"\\nQuery:\", trace.question)\n",
    "for idx, row in enumerate(trace_retrieved, start=1):\n",
    "    print(f\"{idx}. {row.chunk_id} | score={row.score:.4f} | {row.text[:100]}\")\n",
    "print(\"\\nAnswer:\", trace_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
